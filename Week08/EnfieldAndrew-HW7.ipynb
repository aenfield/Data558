{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-23T15:58:00.446415",
     "start_time": "2017-05-23T15:58:00.416914"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# general\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 8, 5\n",
    "\n",
    "# first used in exercise one\n",
    "import linearsvm as svm\n",
    "from sklearn import preprocessing # for scale\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-23T15:58:59.893842",
     "start_time": "2017-05-23T15:58:59.885841"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'linearsvm' from '/Users/andrewenfield/work/github/Data558/Week08/linearsvm.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note: Per the request in the \"Collaboration policy\" note, I've discussed at least part of this assignment with many of the MS employees in the class, including Abhishek, Geoff, Suman, and Charles. (Different weeks/different assignments have different people, depending upon who attends our study groups, but I'll probably just include this blurb w/ each homework since it's generally correct.) I've also gotten input from the discussion board."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Compute the gradient ∇F(β) of F._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gradient](gradient.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Consider the Spam dataset from The Elements of Statistical Learning. Standardize the data, if you have not done so already._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-23T15:42:34.580430",
     "start_time": "2017-05-23T15:42:34.535387"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 58)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam = pd.read_table('data/spam.data', sep=' ', header=None)\n",
    "spam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-23T15:45:14.064578",
     "start_time": "2017-05-23T15:45:14.035080"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7    8     9  ...   48     49  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.0  0.00 ...  0.0  0.000   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.0  0.94 ...  0.0  0.132   \n",
       "\n",
       "    50     51    52     53     54   55    56  57  \n",
       "0  0.0  0.778  0.00  0.000  3.756   61   278   1  \n",
       "1  0.0  0.372  0.18  0.048  5.114  101  1028   1  \n",
       "\n",
       "[2 rows x 58 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and the train/test split info\n",
    "spam_traintest = pd.read_table('data/spam.traintest', header=None, names=['TestIndicator'])\n",
    "spam_traintest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TestIndicator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TestIndicator\n",
       "0              1\n",
       "1              0\n",
       "2              1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_traintest[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert label 0/1 to -1/1\n",
    "spam[57] = spam[57].apply(lambda v: -1 if v == 0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-23T16:00:56.385119",
     "start_time": "2017-05-23T16:00:56.373119"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    2788\n",
       " 1    1813\n",
       "Name: 57, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam[57].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-23T15:44:30.698503",
     "start_time": "2017-05-23T15:44:30.686501"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4601, 57), (4601,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = spam.values[:,0:57]\n",
    "y = spam.values[:,57]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-23T15:51:50.509463",
     "start_time": "2017-05-23T15:51:50.496968"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 57)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled = preprocessing.scale(X)\n",
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-23T15:52:05.561225",
     "start_time": "2017-05-23T15:52:05.552723"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3065, 57), (1536, 57), (3065,), (1536,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled_train = X_scaled[spam_traintest['TestIndicator'] == 0, :]\n",
    "X_scaled_test = X_scaled[spam_traintest['TestIndicator'] == 1, :]\n",
    "y_train = y[spam_traintest['TestIndicator'] == 0]\n",
    "y_test = y[spam_traintest['TestIndicator'] == 1]\n",
    "\n",
    "X_scaled_train.shape, X_scaled_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write a function mylinearsvm that implements the fast gradient algorithm to train the linear support vector machine with the squared hinge loss. The function takes as input the initial step-size value for the backtracking rule and a maximum number of iterations._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I implemented the mylinearsvm function, and all of the supporting functions including the gradient and objective functions, in the file linearsvm.py, which I imported into this notebook with the alias svm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Train your linear support vector machine with the squared hinge loss on the the Spam dataset for the λ = 1. Report your misclassiﬁcation error for this value of λ._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.004722</td>\n",
       "      <td>-0.017823</td>\n",
       "      <td>0.041006</td>\n",
       "      <td>0.022067</td>\n",
       "      <td>0.063296</td>\n",
       "      <td>0.054764</td>\n",
       "      <td>0.113741</td>\n",
       "      <td>0.060633</td>\n",
       "      <td>0.043391</td>\n",
       "      <td>0.024409</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023846</td>\n",
       "      <td>-0.023559</td>\n",
       "      <td>-0.017875</td>\n",
       "      <td>-0.011616</td>\n",
       "      <td>0.057524</td>\n",
       "      <td>0.105725</td>\n",
       "      <td>0.022038</td>\n",
       "      <td>0.024625</td>\n",
       "      <td>0.054172</td>\n",
       "      <td>0.056568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.004746</td>\n",
       "      <td>-0.017818</td>\n",
       "      <td>0.041007</td>\n",
       "      <td>0.022053</td>\n",
       "      <td>0.063268</td>\n",
       "      <td>0.054747</td>\n",
       "      <td>0.113744</td>\n",
       "      <td>0.060613</td>\n",
       "      <td>0.043376</td>\n",
       "      <td>0.024423</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023843</td>\n",
       "      <td>-0.023565</td>\n",
       "      <td>-0.017887</td>\n",
       "      <td>-0.011614</td>\n",
       "      <td>0.057509</td>\n",
       "      <td>0.105748</td>\n",
       "      <td>0.022037</td>\n",
       "      <td>0.024630</td>\n",
       "      <td>0.054171</td>\n",
       "      <td>0.056561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.004766</td>\n",
       "      <td>-0.017812</td>\n",
       "      <td>0.041009</td>\n",
       "      <td>0.022039</td>\n",
       "      <td>0.063242</td>\n",
       "      <td>0.054732</td>\n",
       "      <td>0.113743</td>\n",
       "      <td>0.060593</td>\n",
       "      <td>0.043364</td>\n",
       "      <td>0.024438</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023841</td>\n",
       "      <td>-0.023566</td>\n",
       "      <td>-0.017900</td>\n",
       "      <td>-0.011609</td>\n",
       "      <td>0.057496</td>\n",
       "      <td>0.105773</td>\n",
       "      <td>0.022036</td>\n",
       "      <td>0.024635</td>\n",
       "      <td>0.054170</td>\n",
       "      <td>0.056552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.004783</td>\n",
       "      <td>-0.017804</td>\n",
       "      <td>0.041013</td>\n",
       "      <td>0.022026</td>\n",
       "      <td>0.063219</td>\n",
       "      <td>0.054721</td>\n",
       "      <td>0.113737</td>\n",
       "      <td>0.060574</td>\n",
       "      <td>0.043353</td>\n",
       "      <td>0.024454</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023839</td>\n",
       "      <td>-0.023562</td>\n",
       "      <td>-0.017913</td>\n",
       "      <td>-0.011603</td>\n",
       "      <td>0.057483</td>\n",
       "      <td>0.105801</td>\n",
       "      <td>0.022035</td>\n",
       "      <td>0.024639</td>\n",
       "      <td>0.054168</td>\n",
       "      <td>0.056542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.004795</td>\n",
       "      <td>-0.017797</td>\n",
       "      <td>0.041018</td>\n",
       "      <td>0.022014</td>\n",
       "      <td>0.063200</td>\n",
       "      <td>0.054712</td>\n",
       "      <td>0.113726</td>\n",
       "      <td>0.060555</td>\n",
       "      <td>0.043346</td>\n",
       "      <td>0.024469</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023837</td>\n",
       "      <td>-0.023553</td>\n",
       "      <td>-0.017926</td>\n",
       "      <td>-0.011596</td>\n",
       "      <td>0.057472</td>\n",
       "      <td>0.105830</td>\n",
       "      <td>0.022033</td>\n",
       "      <td>0.024642</td>\n",
       "      <td>0.054167</td>\n",
       "      <td>0.056531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "96   0.004722 -0.017823  0.041006  0.022067  0.063296  0.054764  0.113741   \n",
       "97   0.004746 -0.017818  0.041007  0.022053  0.063268  0.054747  0.113744   \n",
       "98   0.004766 -0.017812  0.041009  0.022039  0.063242  0.054732  0.113743   \n",
       "99   0.004783 -0.017804  0.041013  0.022026  0.063219  0.054721  0.113737   \n",
       "100  0.004795 -0.017797  0.041018  0.022014  0.063200  0.054712  0.113726   \n",
       "\n",
       "           7         8         9     ...           47        48        49  \\\n",
       "96   0.060633  0.043391  0.024409    ...    -0.023846 -0.023559 -0.017875   \n",
       "97   0.060613  0.043376  0.024423    ...    -0.023843 -0.023565 -0.017887   \n",
       "98   0.060593  0.043364  0.024438    ...    -0.023841 -0.023566 -0.017900   \n",
       "99   0.060574  0.043353  0.024454    ...    -0.023839 -0.023562 -0.017913   \n",
       "100  0.060555  0.043346  0.024469    ...    -0.023837 -0.023553 -0.017926   \n",
       "\n",
       "           50        51        52        53        54        55        56  \n",
       "96  -0.011616  0.057524  0.105725  0.022038  0.024625  0.054172  0.056568  \n",
       "97  -0.011614  0.057509  0.105748  0.022037  0.024630  0.054171  0.056561  \n",
       "98  -0.011609  0.057496  0.105773  0.022036  0.024635  0.054170  0.056552  \n",
       "99  -0.011603  0.057483  0.105801  0.022035  0.024639  0.054168  0.056542  \n",
       "100 -0.011596  0.057472  0.105830  0.022033  0.024642  0.054167  0.056531  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = svm.fastgradalgo(\n",
    "    X_scaled_train, y_train, t_init=0.01, \n",
    "    grad_func = svm.compute_linearsvm_gradient, \n",
    "    obj_func = svm.compute_linearsvm_objective, \n",
    "    lam=1)\n",
    "results[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00479497, -0.0177966 ,  0.04101844,  0.02201367,  0.0632002 ,\n",
       "         0.05471219,  0.11372629,  0.06055507,  0.04334626,  0.0244685 ,\n",
       "         0.04295497, -0.01973514,  0.01350283,  0.01194526,  0.03780159,\n",
       "         0.10502347,  0.06651882,  0.05194878,  0.05429956,  0.04372403,\n",
       "         0.09503506,  0.04485843,  0.09438071,  0.06443725, -0.05358436,\n",
       "        -0.03836966, -0.04613438, -0.01988765, -0.01862775, -0.02545586,\n",
       "        -0.00798179, -0.0042151 , -0.02998457, -0.00438627, -0.01557435,\n",
       "        -0.00621837, -0.03105852, -0.01273458, -0.02824564,  0.00899199,\n",
       "        -0.01605101, -0.03340787, -0.02412678, -0.02500915, -0.04566112,\n",
       "        -0.04228721, -0.01269574, -0.02383744, -0.02355306, -0.01792578,\n",
       "        -0.01159551,  0.05747161,  0.10582991,  0.02203296,  0.0246417 ,\n",
       "         0.05416728,  0.05653074]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.get_final_coefs(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_accuracy(beta_coefs, X, y, threshold=0):\n",
    "    y_pred = X.dot(beta_coefs.T).ravel() # ravel to convert to vector\n",
    "    # convert to -1 or +1 depending on threshold\n",
    "    y_thresholded = np.where(y_pred > threshold, 1, -1)\n",
    "    return accuracy_score(y, y_thresholded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.4%\n",
      "Misclassification error: 9.6%\n"
     ]
    }
   ],
   "source": [
    "# note use of the held out test data to get the performance metrics\n",
    "accuracy = get_accuracy(svm.get_final_coefs(results), X_scaled_test, y_test)\n",
    "print(\"Accuracy: {0:.1%}\".format(accuracy))\n",
    "print(\"Misclassification error: {0:.1%}\".format(1 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** ? try with a basic sklearn impl and see if coefs and accuracy are close? not needed for this assignment, but could be worthwhile since I'll be using this fastgradalgo impl for both the polished code release and for the final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Run cross-validation to ﬁnd the optimal value of λ. Report your misclassiﬁcation error for that value of λ._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_test_single_fold(X_full, y_full, lam, train_index, test_index):\n",
    "    \"\"\"\n",
    "    Train using the data identified by the indices in train_index, and then test\n",
    "    (and return accuracy) using the data identified by the indices in test_index.\n",
    "    \"\"\"\n",
    "    beta_vals = svm.fastgradalgo(\n",
    "        X_full[train_index], y_full[train_index], t_init=0.01, \n",
    "        grad_func = svm.compute_linearsvm_gradient, \n",
    "        obj_func = svm.compute_linearsvm_objective, \n",
    "        lam=lam)\n",
    "\n",
    "#     beta_vals = mt.randcoorddescent(X_full[train_index], y_full[train_index], \n",
    "#                                     lam, alpha, max_iter=500, beta0=beta0)\n",
    "\n",
    "    final_coefs = svm.get_final_coefs(beta_vals)\n",
    "    \n",
    "    return get_accuracy(final_coefs, X_full[test_index], y_full[test_index])\n",
    "    \n",
    "#     return mean_squared_error(y_full[test_index], \n",
    "#                               X_full[test_index].dot(final_coefs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_test_for_all_folds(X_full, y_full, train_indices, \n",
    "                                 test_indices, lam):\n",
    "    \"\"\"\n",
    "    Train and test for all folds - for now, 10 folds, hard-coded. Return \n",
    "    the mean of the set of accuracy scores from all folds.\"\"\"\n",
    "    accuracy_scores = [train_and_test_single_fold(X_full, y_full, lam,\n",
    "                                       train_indices[i], \n",
    "                                       test_indices[i]) for i in range(10)]\n",
    "    return(np.mean(accuracy_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get arrays with 10 sets of test and train indices - one for each fold\n",
    "kf = KFold(10, shuffle=True, random_state=42)\n",
    "\n",
    "train_indices_list = []\n",
    "test_indices_list = []\n",
    "for train_index, test_index in kf.split(X_scaled_train):\n",
    "    train_indices_list.append(train_index)\n",
    "    test_indices_list.append(test_index)\n",
    "    \n",
    "train_indices = np.array(train_indices_list)\n",
    "test_indices = np.array(test_indices_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1e-10, 1e-09, 1e-08, 1e-07, 1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas = [10 ** exponent for exponent in range(-10,2)]\n",
    "lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lambdas = np.logspace(-5, 0.1, num=10)\n",
    "# lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1e-10, 0.91322198803517074),\n",
       " (1e-09, 0.91322198803517074),\n",
       " (1e-08, 0.91322198803517074),\n",
       " (1e-07, 0.91322198803517074),\n",
       " (1e-06, 0.91322198803517074),\n",
       " (1e-05, 0.91322198803517074),\n",
       " (0.0001, 0.91322198803517074),\n",
       " (0.001, 0.91289412616295151),\n",
       " (0.01, 0.91093972876881479),\n",
       " (0.1, 0.91093334184922603),\n",
       " (1, 0.90929083902833663),\n",
       " (10, 0.8939569095825084)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and finally, do 10-fold cross validation for each value of lambda, and\n",
    "# show the mean of each set's MSEs\n",
    "accuracy_values_by_lambda = [train_and_test_for_all_folds(X_scaled_train, y_train, \n",
    "                                train_indices, test_indices, \n",
    "                                lam) for lam in lambdas]\n",
    "list(zip(lambdas, accuracy_values_by_lambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** why would we be getting the same low misclassification error with so many of the low lambda values? Try with diff maxiters to see if it changes things? Try with othere param changes? Think about it generally and see what else could be causing this? Would it make sense that lambda should basically be zero? So we don't penalize anything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_lambda = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-0.004560</td>\n",
       "      <td>-0.024520</td>\n",
       "      <td>0.053345</td>\n",
       "      <td>0.168683</td>\n",
       "      <td>0.098476</td>\n",
       "      <td>0.071190</td>\n",
       "      <td>0.379080</td>\n",
       "      <td>0.096020</td>\n",
       "      <td>0.051182</td>\n",
       "      <td>0.021623</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.130500</td>\n",
       "      <td>-0.142286</td>\n",
       "      <td>-0.069444</td>\n",
       "      <td>-0.026484</td>\n",
       "      <td>0.078290</td>\n",
       "      <td>0.605650</td>\n",
       "      <td>0.164150</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>0.272586</td>\n",
       "      <td>0.105239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-0.004803</td>\n",
       "      <td>-0.024611</td>\n",
       "      <td>0.053362</td>\n",
       "      <td>0.170677</td>\n",
       "      <td>0.098611</td>\n",
       "      <td>0.071314</td>\n",
       "      <td>0.377722</td>\n",
       "      <td>0.096268</td>\n",
       "      <td>0.051379</td>\n",
       "      <td>0.021627</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.131481</td>\n",
       "      <td>-0.143140</td>\n",
       "      <td>-0.069599</td>\n",
       "      <td>-0.026641</td>\n",
       "      <td>0.078389</td>\n",
       "      <td>0.608849</td>\n",
       "      <td>0.165773</td>\n",
       "      <td>0.052071</td>\n",
       "      <td>0.273975</td>\n",
       "      <td>0.105322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-0.005029</td>\n",
       "      <td>-0.024698</td>\n",
       "      <td>0.053376</td>\n",
       "      <td>0.172679</td>\n",
       "      <td>0.098738</td>\n",
       "      <td>0.071425</td>\n",
       "      <td>0.376326</td>\n",
       "      <td>0.096513</td>\n",
       "      <td>0.051553</td>\n",
       "      <td>0.021634</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.132447</td>\n",
       "      <td>-0.143984</td>\n",
       "      <td>-0.069753</td>\n",
       "      <td>-0.026791</td>\n",
       "      <td>0.078482</td>\n",
       "      <td>0.611983</td>\n",
       "      <td>0.167397</td>\n",
       "      <td>0.050609</td>\n",
       "      <td>0.275349</td>\n",
       "      <td>0.105410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-0.005238</td>\n",
       "      <td>-0.024781</td>\n",
       "      <td>0.053385</td>\n",
       "      <td>0.174689</td>\n",
       "      <td>0.098857</td>\n",
       "      <td>0.071523</td>\n",
       "      <td>0.374894</td>\n",
       "      <td>0.096753</td>\n",
       "      <td>0.051702</td>\n",
       "      <td>0.021641</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133399</td>\n",
       "      <td>-0.144815</td>\n",
       "      <td>-0.069908</td>\n",
       "      <td>-0.026933</td>\n",
       "      <td>0.078567</td>\n",
       "      <td>0.615051</td>\n",
       "      <td>0.169022</td>\n",
       "      <td>0.049117</td>\n",
       "      <td>0.276707</td>\n",
       "      <td>0.105503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>-0.005428</td>\n",
       "      <td>-0.024858</td>\n",
       "      <td>0.053391</td>\n",
       "      <td>0.176707</td>\n",
       "      <td>0.098968</td>\n",
       "      <td>0.071606</td>\n",
       "      <td>0.373429</td>\n",
       "      <td>0.096984</td>\n",
       "      <td>0.051826</td>\n",
       "      <td>0.021649</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134336</td>\n",
       "      <td>-0.145633</td>\n",
       "      <td>-0.070062</td>\n",
       "      <td>-0.027067</td>\n",
       "      <td>0.078642</td>\n",
       "      <td>0.618054</td>\n",
       "      <td>0.170650</td>\n",
       "      <td>0.047593</td>\n",
       "      <td>0.278051</td>\n",
       "      <td>0.105600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "96  -0.004560 -0.024520  0.053345  0.168683  0.098476  0.071190  0.379080   \n",
       "97  -0.004803 -0.024611  0.053362  0.170677  0.098611  0.071314  0.377722   \n",
       "98  -0.005029 -0.024698  0.053376  0.172679  0.098738  0.071425  0.376326   \n",
       "99  -0.005238 -0.024781  0.053385  0.174689  0.098857  0.071523  0.374894   \n",
       "100 -0.005428 -0.024858  0.053391  0.176707  0.098968  0.071606  0.373429   \n",
       "\n",
       "           7         8         9     ...           47        48        49  \\\n",
       "96   0.096020  0.051182  0.021623    ...    -0.130500 -0.142286 -0.069444   \n",
       "97   0.096268  0.051379  0.021627    ...    -0.131481 -0.143140 -0.069599   \n",
       "98   0.096513  0.051553  0.021634    ...    -0.132447 -0.143984 -0.069753   \n",
       "99   0.096753  0.051702  0.021641    ...    -0.133399 -0.144815 -0.069908   \n",
       "100  0.096984  0.051826  0.021649    ...    -0.134336 -0.145633 -0.070062   \n",
       "\n",
       "           50        51        52        53        54        55        56  \n",
       "96  -0.026484  0.078290  0.605650  0.164150  0.053500  0.272586  0.105239  \n",
       "97  -0.026641  0.078389  0.608849  0.165773  0.052071  0.273975  0.105322  \n",
       "98  -0.026791  0.078482  0.611983  0.167397  0.050609  0.275349  0.105410  \n",
       "99  -0.026933  0.078567  0.615051  0.169022  0.049117  0.276707  0.105503  \n",
       "100 -0.027067  0.078642  0.618054  0.170650  0.047593  0.278051  0.105600  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train with best lambda\n",
    "results_best = svm.fastgradalgo(\n",
    "    X_scaled_train, y_train, t_init=0.01, \n",
    "    grad_func = svm.compute_linearsvm_gradient, \n",
    "    obj_func = svm.compute_linearsvm_objective, \n",
    "    lam=best_lambda)\n",
    "results_best[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.5%\n",
      "Misclassification error: 8.5%\n"
     ]
    }
   ],
   "source": [
    "# note use of the held out test data to get the performance metrics\n",
    "accuracy = get_accuracy(svm.get_final_coefs(results_best), X_scaled_test, y_test)\n",
    "print(\"Accuracy: {0:.1%}\".format(accuracy))\n",
    "print(\"Misclassification error: {0:.1%}\".format(1 - accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
