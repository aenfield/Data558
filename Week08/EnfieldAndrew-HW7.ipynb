{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-23T15:58:00.446415",
     "start_time": "2017-05-23T15:58:00.416914"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# general\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 8, 5\n",
    "\n",
    "# first used in exercise one\n",
    "import linearsvm as svm\n",
    "from sklearn import preprocessing # for scale\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-23T15:58:59.893842",
     "start_time": "2017-05-23T15:58:59.885841"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'linearsvm' from '/Users/andrewenfield/work/github/Data558/Week08/linearsvm.py'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note: Per the request in the \"Collaboration policy\" note, I've discussed at least part of this assignment with many of the MS employees in the class, including Abhishek, Geoff, Suman, and Charles. (Different weeks/different assignments have different people, depending upon who attends our study groups, but I'll probably just include this blurb w/ each homework since it's generally correct.) I've also gotten input from the discussion board."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Compute the gradient ∇F(β) of F._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gradient](gradient.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Consider the Spam dataset from The Elements of Statistical Learning. Standardize the data, if you have not done so already._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-23T15:42:34.580430",
     "start_time": "2017-05-23T15:42:34.535387"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 58)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam = pd.read_table('data/spam.data', sep=' ', header=None)\n",
    "spam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-23T15:45:14.064578",
     "start_time": "2017-05-23T15:45:14.035080"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7    8     9  ...   48     49  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.0  0.00 ...  0.0  0.000   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.0  0.94 ...  0.0  0.132   \n",
       "\n",
       "    50     51    52     53     54   55    56  57  \n",
       "0  0.0  0.778  0.00  0.000  3.756   61   278   1  \n",
       "1  0.0  0.372  0.18  0.048  5.114  101  1028   1  \n",
       "\n",
       "[2 rows x 58 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2788\n",
       "1    1813\n",
       "Name: 57, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam[57].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert 0/1 to -1/1\n",
    "spam[57] = spam[57].apply(lambda v: -1 if v == 0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-23T16:00:56.385119",
     "start_time": "2017-05-23T16:00:56.373119"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    2788\n",
       " 1    1813\n",
       "Name: 57, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam[57].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-23T15:44:30.698503",
     "start_time": "2017-05-23T15:44:30.686501"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4601, 57), (4601,))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = spam.values[:,0:57]\n",
    "y = spam.values[:,57]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-23T15:51:50.509463",
     "start_time": "2017-05-23T15:51:50.496968"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 57)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled = preprocessing.scale(X)\n",
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-23T15:52:05.561225",
     "start_time": "2017-05-23T15:52:05.552723"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.34243371,  0.3308849 ,  0.71285877, -0.04689958,  0.01156471,\n",
       "        -0.35026618, -0.29179389, -0.26256156, -0.32330236, -0.37136439,\n",
       "        -0.29685953,  0.11408733, -0.31205521, -0.17492717, -0.19011441,\n",
       "         0.08617144, -0.32113541,  2.08120664,  0.15090537, -0.16789311,\n",
       "         0.1251284 , -0.11817151, -0.2902092 , -0.21299439, -0.32881467,\n",
       "        -0.29923993, -0.22789481, -0.23183016, -0.16673145, -0.22523952,\n",
       "        -0.16053931, -0.14321202, -0.17492026, -0.14521515, -0.19806739,\n",
       "        -0.24213022, -0.32345561, -0.05983624, -0.18091134, -0.18530385,\n",
       "        -0.12090468, -0.17259996, -0.20599311, -0.12734332, -0.29777621,\n",
       "        -0.19738748, -0.0713879 , -0.11154623, -0.15845336, -0.51430655,\n",
       "        -0.15519768,  0.62400658, -0.30835494, -0.1030484 , -0.04524728,\n",
       "         0.04529792, -0.00872413]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write a function mylinearsvm that implements the fast gradient algorithm to train the linear support vector machine with the squared hinge loss. The function takes as input the initial step-size value for the backtracking rule and a maximum number of iterations._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I implemented the mylinearsvm function, and all of the supporting functions including the gradient and objective functions, in the file linearsvm.py, which I imported into this notebook with the alias svm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Train your linear support vector machine with the squared hinge loss on the the Spam dataset for the λ = 1. Report your misclassiﬁcation error for this value of λ._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.004773</td>\n",
       "      <td>-0.013833</td>\n",
       "      <td>0.034621</td>\n",
       "      <td>0.021870</td>\n",
       "      <td>0.070751</td>\n",
       "      <td>0.052163</td>\n",
       "      <td>0.114484</td>\n",
       "      <td>0.057632</td>\n",
       "      <td>0.047464</td>\n",
       "      <td>0.025636</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022830</td>\n",
       "      <td>-0.025820</td>\n",
       "      <td>-0.012043</td>\n",
       "      <td>-0.011067</td>\n",
       "      <td>0.068568</td>\n",
       "      <td>0.101598</td>\n",
       "      <td>0.023084</td>\n",
       "      <td>0.025797</td>\n",
       "      <td>0.056279</td>\n",
       "      <td>0.063825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.004795</td>\n",
       "      <td>-0.013829</td>\n",
       "      <td>0.034627</td>\n",
       "      <td>0.021865</td>\n",
       "      <td>0.070724</td>\n",
       "      <td>0.052143</td>\n",
       "      <td>0.114499</td>\n",
       "      <td>0.057609</td>\n",
       "      <td>0.047451</td>\n",
       "      <td>0.025639</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022822</td>\n",
       "      <td>-0.025824</td>\n",
       "      <td>-0.012051</td>\n",
       "      <td>-0.011070</td>\n",
       "      <td>0.068546</td>\n",
       "      <td>0.101618</td>\n",
       "      <td>0.023089</td>\n",
       "      <td>0.025803</td>\n",
       "      <td>0.056276</td>\n",
       "      <td>0.063813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.004815</td>\n",
       "      <td>-0.013825</td>\n",
       "      <td>0.034634</td>\n",
       "      <td>0.021860</td>\n",
       "      <td>0.070697</td>\n",
       "      <td>0.052127</td>\n",
       "      <td>0.114510</td>\n",
       "      <td>0.057587</td>\n",
       "      <td>0.047441</td>\n",
       "      <td>0.025644</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022816</td>\n",
       "      <td>-0.025823</td>\n",
       "      <td>-0.012061</td>\n",
       "      <td>-0.011072</td>\n",
       "      <td>0.068525</td>\n",
       "      <td>0.101640</td>\n",
       "      <td>0.023095</td>\n",
       "      <td>0.025809</td>\n",
       "      <td>0.056275</td>\n",
       "      <td>0.063798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.004834</td>\n",
       "      <td>-0.013819</td>\n",
       "      <td>0.034642</td>\n",
       "      <td>0.021853</td>\n",
       "      <td>0.070672</td>\n",
       "      <td>0.052116</td>\n",
       "      <td>0.114516</td>\n",
       "      <td>0.057567</td>\n",
       "      <td>0.047434</td>\n",
       "      <td>0.025650</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022811</td>\n",
       "      <td>-0.025817</td>\n",
       "      <td>-0.012072</td>\n",
       "      <td>-0.011071</td>\n",
       "      <td>0.068505</td>\n",
       "      <td>0.101663</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.025815</td>\n",
       "      <td>0.056274</td>\n",
       "      <td>0.063782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.004849</td>\n",
       "      <td>-0.013813</td>\n",
       "      <td>0.034652</td>\n",
       "      <td>0.021845</td>\n",
       "      <td>0.070649</td>\n",
       "      <td>0.052109</td>\n",
       "      <td>0.114517</td>\n",
       "      <td>0.057549</td>\n",
       "      <td>0.047430</td>\n",
       "      <td>0.025657</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022808</td>\n",
       "      <td>-0.025807</td>\n",
       "      <td>-0.012084</td>\n",
       "      <td>-0.011070</td>\n",
       "      <td>0.068488</td>\n",
       "      <td>0.101688</td>\n",
       "      <td>0.023105</td>\n",
       "      <td>0.025819</td>\n",
       "      <td>0.056273</td>\n",
       "      <td>0.063765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "96   0.004773 -0.013833  0.034621  0.021870  0.070751  0.052163  0.114484   \n",
       "97   0.004795 -0.013829  0.034627  0.021865  0.070724  0.052143  0.114499   \n",
       "98   0.004815 -0.013825  0.034634  0.021860  0.070697  0.052127  0.114510   \n",
       "99   0.004834 -0.013819  0.034642  0.021853  0.070672  0.052116  0.114516   \n",
       "100  0.004849 -0.013813  0.034652  0.021845  0.070649  0.052109  0.114517   \n",
       "\n",
       "           7         8         9     ...           47        48        49  \\\n",
       "96   0.057632  0.047464  0.025636    ...    -0.022830 -0.025820 -0.012043   \n",
       "97   0.057609  0.047451  0.025639    ...    -0.022822 -0.025824 -0.012051   \n",
       "98   0.057587  0.047441  0.025644    ...    -0.022816 -0.025823 -0.012061   \n",
       "99   0.057567  0.047434  0.025650    ...    -0.022811 -0.025817 -0.012072   \n",
       "100  0.057549  0.047430  0.025657    ...    -0.022808 -0.025807 -0.012084   \n",
       "\n",
       "           50        51        52        53        54        55        56  \n",
       "96  -0.011067  0.068568  0.101598  0.023084  0.025797  0.056279  0.063825  \n",
       "97  -0.011070  0.068546  0.101618  0.023089  0.025803  0.056276  0.063813  \n",
       "98  -0.011072  0.068525  0.101640  0.023095  0.025809  0.056275  0.063798  \n",
       "99  -0.011071  0.068505  0.101663  0.023100  0.025815  0.056274  0.063782  \n",
       "100 -0.011070  0.068488  0.101688  0.023105  0.025819  0.056273  0.063765  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = svm.fastgradalgo(\n",
    "    X_scaled, y, t_init=0.01, \n",
    "    grad_func = svm.compute_linearsvm_gradient, \n",
    "    obj_func = svm.compute_linearsvm_objective, \n",
    "    lam=1)\n",
    "results[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00484856, -0.01381278,  0.03465152,  0.02184484,  0.07064941,\n",
       "         0.05210909,  0.11451729,  0.05754886,  0.04742959,  0.02565736,\n",
       "         0.0395877 , -0.02027215,  0.01739698,  0.00703854,  0.03922624,\n",
       "         0.08387392,  0.06459502,  0.05084026,  0.04901419,  0.04912297,\n",
       "         0.08789593,  0.04203993,  0.09554004,  0.06106319, -0.05455013,\n",
       "        -0.03976008, -0.0484804 , -0.01311055, -0.01641889, -0.02634323,\n",
       "        -0.01025193, -0.00084591, -0.03294699, -0.00048894, -0.02046245,\n",
       "        -0.00402314, -0.03110056, -0.01247296, -0.02326556,  0.0067796 ,\n",
       "        -0.01671461, -0.03714934, -0.02274022, -0.02592255, -0.04297767,\n",
       "        -0.04192052, -0.01662661, -0.02280837, -0.0258065 , -0.01208367,\n",
       "        -0.01106978,  0.0684883 ,  0.1016875 ,  0.02310485,  0.02581931,\n",
       "         0.05627304,  0.06376549]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.get_final_coefs(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_accuracy(beta_coefs, X, y, threshold=0):\n",
    "    y_pred = X.dot(beta_coefs.T).ravel() # ravel to convert to vector\n",
    "    # convert to -1 or +1 depending on threshold\n",
    "    y_thresholded = np.where(y_pred > threshold, 1, -1)\n",
    "    return accuracy_score(y, y_thresholded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.1%\n",
      "Misclassification error: 8.9%\n"
     ]
    }
   ],
   "source": [
    "accuracy = get_accuracy(svm.get_final_coefs(results), X_scaled, y)\n",
    "print(\"Accuracy: {0:.1%}\".format(accuracy))\n",
    "print(\"Misclassification error: {0:.1%}\".format(1 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** ? try with a basic sklearn impl and see if coefs and accuracy are close?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Run cross-validation to ﬁnd the optimal value of λ. Report your misclassiﬁcation error for that value of λ._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_test_single_fold(X_full, y_full, lam, train_index, test_index):\n",
    "    \"\"\"\n",
    "    Train using the data identified by the indices in train_index, and then test\n",
    "    (and return accuracy) using the data identified by the indices in test_index.\n",
    "    \"\"\"\n",
    "    beta_vals = svm.fastgradalgo(\n",
    "        X_full[train_index], y_full[train_index], t_init=0.01, \n",
    "        grad_func = svm.compute_linearsvm_gradient, \n",
    "        obj_func = svm.compute_linearsvm_objective, \n",
    "        lam=lam)\n",
    "\n",
    "#     beta_vals = mt.randcoorddescent(X_full[train_index], y_full[train_index], \n",
    "#                                     lam, alpha, max_iter=500, beta0=beta0)\n",
    "\n",
    "    final_coefs = svm.get_final_coefs(beta_vals)\n",
    "    \n",
    "    return get_accuracy(final_coefs, X_full[test_index], y_full[test_index])\n",
    "    \n",
    "#     return mean_squared_error(y_full[test_index], \n",
    "#                               X_full[test_index].dot(final_coefs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_test_for_all_folds(X_full, y_full, train_indices, \n",
    "                                 test_indices, lam):\n",
    "    \"\"\"\n",
    "    Train and test for all folds - for now, 10 folds, hard-coded. Return \n",
    "    the mean of the set of accuracy scores from all folds.\"\"\"\n",
    "    accuracy_scores = [train_and_test_single_fold(X_full, y_full, lam,\n",
    "                                       train_indices[i], \n",
    "                                       test_indices[i]) for i in range(10)]\n",
    "    return(np.mean(accuracy_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get arrays with 10 sets of test and train indices - one for each fold\n",
    "kf = KFold(10, shuffle=True, random_state=42)\n",
    "\n",
    "train_indices_list = []\n",
    "test_indices_list = []\n",
    "for train_index, test_index in kf.split(X_scaled):\n",
    "    train_indices_list.append(train_index)\n",
    "    test_indices_list.append(test_index)\n",
    "    \n",
    "train_indices = np.array(train_indices_list)\n",
    "test_indices = np.array(test_indices_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1e-15,\n",
       " 1e-14,\n",
       " 1e-13,\n",
       " 1e-12,\n",
       " 1e-11,\n",
       " 1e-10,\n",
       " 1e-09,\n",
       " 1e-08,\n",
       " 1e-07,\n",
       " 1e-06,\n",
       " 1e-05,\n",
       " 0.0001,\n",
       " 0.001,\n",
       " 0.01,\n",
       " 0.1,\n",
       " 1,\n",
       " 10,\n",
       " 100]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas = [10 ** exponent for exponent in range(-15,3)]\n",
    "lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lambdas = np.logspace(-5, 0.1, num=10)\n",
    "# lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1e-15, 0.9169777421484484),\n",
       " (1e-14, 0.9169777421484484),\n",
       " (1e-13, 0.9169777421484484),\n",
       " (1e-12, 0.9169777421484484),\n",
       " (1e-11, 0.9169777421484484),\n",
       " (1e-10, 0.9169777421484484),\n",
       " (1e-09, 0.9169777421484484),\n",
       " (1e-08, 0.9169777421484484),\n",
       " (1e-07, 0.9169777421484484),\n",
       " (1e-06, 0.9169777421484484),\n",
       " (1e-05, 0.9169777421484484),\n",
       " (0.0001, 0.9169777421484484),\n",
       " (0.001, 0.91719513345279624),\n",
       " (0.01, 0.91502169197396965),\n",
       " (0.1, 0.91263085919079501),\n",
       " (1, 0.91001839102140902),\n",
       " (10, 0.89567763840422532),\n",
       " (100, 0.35036782042818071)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and finally, do 10-fold cross validation for each value of lambda, and\n",
    "# show the mean of each set's MSEs\n",
    "accuracy_values_by_lambda = [train_and_test_for_all_folds(X_scaled, y, \n",
    "                                train_indices, test_indices, \n",
    "                                lam) for lam in lambdas]\n",
    "list(zip(lambdas, accuracy_values_by_lambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** why would we be getting the same low misclassification error with so many of the low lambda values? Try with diff maxiters to see if it changes things? Try with othere param changes? Think about it generally and see what else could be causing this? Would it make sense that lambda should basically be zero? So we don't penalize anything?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** calc accuracy and misclassification error with best lambda. should I do this with all of the data? Prob not. I should prob do it with a holdout set. If the latter's true, should I be holding out that set above? Think it through and implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
