{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# general\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 8, 5\n",
    "\n",
    "# first used in exercise two\n",
    "import pca\n",
    "from sklearn import preprocessing # for scale\n",
    "\n",
    "# first used in exercise three\n",
    "import coorddescent as cd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pca' from '/Users/andrewenfield/work/github/Data558/Week05/pca.py'>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Per the request in the \"Collaboration policy\" note, I've discussed at least part of this assignment with many of the MS employees in the class, including Amitabh, Abhishek, Geoff, Suman, Charles, and Salik. (Different weeks/different assignments have different people, depending upon who attends our study groups, but I'll probably just include this blurb w/ each homework since it's generally correct.) I've also gotten input from the discussion board."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise One"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*\"(a) Generate a simulated data set with 20 observations in each of three classes (i.e. 60 observations total), and 50 variables. Hint: There are a number of functions in numpy that you can use to generate data. One example is the numpy.random.normal() func- tion; numpy.random.uniform() is another option. Be sure to add a mean shift to the observations in each class so that there are three distinct classes.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick note: I had a hard time getting the algorithm to converge when I used very small mean shifts of 0.5 - I tried a lot of different step size value combinations and got nothing but non-convergence. I was able to get it to converge with a larger mean shift, so I'm going with that larger mean shift here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alejandro Toledo' 'Alvaro Uribe' 'Andre Agassi' 'Ariel Sharon'\n",
      " 'Arnold Schwarzenegger' 'Colin Powell' 'David Beckham' 'Donald Rumsfeld'\n",
      " 'George W Bush' 'Gerhard Schroeder' 'Gloria Macapagal Arroyo'\n",
      " 'Guillermo Coria' 'Hans Blix' 'Hugo Chavez' 'Jacques Chirac'\n",
      " 'Jean Chretien' 'Jennifer Capriati' 'John Ashcroft' 'John Negroponte'\n",
      " 'Junichiro Koizumi' 'Kofi Annan' 'Laura Bush' 'Lleyton Hewitt'\n",
      " 'Luiz Inacio Lula da Silva' 'Megawati Sukarnoputri' 'Nestor Kirchner'\n",
      " 'Recep Tayyip Erdogan' 'Roh Moo-hyun' 'Serena Williams'\n",
      " 'Silvio Berlusconi' 'Tom Ridge' 'Tony Blair' 'Vicente Fox'\n",
      " 'Vladimir Putin']\n",
      "(2370, 62, 47)\n",
      "(2370, 2914)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "faces = fetch_lfw_people(min_faces_per_person=30) \n",
    "print(faces.target_names)  # Images of 34 different people\n",
    "print(faces.images.shape)  # 2370 images, each of which is 62x47 pixels\n",
    "print(faces.data.shape)    # Each image is unravelled to a vector of size 2914"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>0.496714</td>\n",
       "      <td>-0.138264</td>\n",
       "      <td>0.647689</td>\n",
       "      <td>1.52303</td>\n",
       "      <td>-0.234153</td>\n",
       "      <td>-0.234137</td>\n",
       "      <td>1.579213</td>\n",
       "      <td>0.767435</td>\n",
       "      <td>-0.469474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.738467</td>\n",
       "      <td>0.171368</td>\n",
       "      <td>-0.115648</td>\n",
       "      <td>-0.301104</td>\n",
       "      <td>-1.478522</td>\n",
       "      <td>-0.719844</td>\n",
       "      <td>-0.460639</td>\n",
       "      <td>1.057122</td>\n",
       "      <td>0.343618</td>\n",
       "      <td>-1.76304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Class         0         1         2        3         4         5         6  \\\n",
       "0     A  0.496714 -0.138264  0.647689  1.52303 -0.234153 -0.234137  1.579213   \n",
       "\n",
       "          7         8   ...           40        41        42        43  \\\n",
       "0  0.767435 -0.469474   ...     0.738467  0.171368 -0.115648 -0.301104   \n",
       "\n",
       "         44        45        46        47        48       49  \n",
       "0 -1.478522 -0.719844 -0.460639  1.057122  0.343618 -1.76304  \n",
       "\n",
       "[1 rows x 51 columns]"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_vals = [0, 10, 20]\n",
    "np.random.seed(42)\n",
    "d = pd.DataFrame(np.vstack([np.random.normal(mean, \n",
    "                        size=(20,50)) for mean in mean_vals]))\n",
    "d.insert(0, 'Class', np.repeat(['A','B','C'], 20))\n",
    "d[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -9.47652431, -10.36087989,  -9.29538686,  -8.58559277,\n",
       "       -10.16925639, -10.28390443,  -8.43284888,  -9.41223556,\n",
       "       -10.46780705,  -9.85403586, -10.39694444, -10.58329323,\n",
       "        -9.84789351, -11.77688128, -12.0374925 , -10.61884412,\n",
       "       -11.00006027,  -9.6519797 , -10.75006323, -11.58583051,\n",
       "        -8.81179199, -10.29558276,  -9.91366629, -11.39948789,\n",
       "       -10.70784122,  -9.85670811, -11.13090521,  -9.8084079 ,\n",
       "       -10.73701623, -10.46793313, -10.56025456,  -7.99222798,\n",
       "        -9.95515398, -11.09243514,  -9.20584621, -11.29344291,\n",
       "        -9.73924429, -11.91219899, -11.31890736,  -9.77094352,\n",
       "        -9.32654553,  -9.90398641, -10.16759232, -10.2470722 ,\n",
       "       -11.37186667, -10.72705805, -10.23186415,  -8.66765149,\n",
       "        -9.90986555, -11.76248424])"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_values = d.values[:, 1:51].astype('float')\n",
    "d_values_centered = preprocessing.scale(d_values, with_std=False)\n",
    "d_values_centered[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X = faces.data\n",
    "#eta_0, t_0, epoch_count = .00001, 1, 50\n",
    "\n",
    "X = d_values\n",
    "eta_0, t_0, epoch_count = 0.001, 2, 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*\"(b) Run your own normalized Oja algorithm on the 60 observations. You should try to implement the algorithm yourself, but you may look back at the labs if you need help. Plot the first two principal component score vectors. Compare your results to the ones obtained with scikit-learn’s PCA algorithm. Use a different color to indicate the observations in each of the three classes. If the three classes appear separated in this plot, then you’re done and you can launch the computations on AWS. If not, then return to part (a) and modify the simulation so that there is greater separation between the three classes.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#bar = pca.oja_fit(d_values_centered_copy, 1, 0.001, 2, 100)\n",
    "#bar[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.14233391 -0.14325963 -0.1412039  -0.13801734 -0.13977184 -0.14364433\n",
      " -0.13991273 -0.14104493 -0.14228272 -0.14028026 -0.13925715 -0.14081765\n",
      " -0.1409513  -0.13850097 -0.1433206  -0.14209824 -0.13842943 -0.13647746\n",
      " -0.14296263 -0.14078382 -0.1406264  -0.14656184 -0.13859031 -0.14020859\n",
      " -0.14084239 -0.14366725 -0.14535678 -0.14170943 -0.14213593 -0.14141145\n",
      " -0.14192793 -0.14247393 -0.14370843 -0.14302147 -0.13910328 -0.14296752\n",
      " -0.14035279 -0.14268592 -0.13854042 -0.14386981 -0.14474806 -0.13853575\n",
      " -0.14457845 -0.13896695 -0.14065459 -0.14347999 -0.1421975  -0.13715082\n",
      " -0.14139857 -0.14340664]\n",
      "[-0.14820248 -0.15577997 -0.02747821  0.05217483  0.09001604  0.21697314\n",
      "  0.04838229  0.0579049   0.1868853   0.07290217 -0.1472483   0.10724179\n",
      " -0.21673595 -0.23128117  0.08330134  0.00877515  0.22128086  0.03809422\n",
      "  0.00873939 -0.0690226   0.24575393  0.15597047  0.08329307 -0.00890795\n",
      " -0.25341302  0.06472497 -0.00552899  0.02767339 -0.23068174 -0.2512671\n",
      "  0.02564334  0.17065277  0.19201366 -0.20402114  0.12081652 -0.03829716\n",
      "  0.02052802 -0.00479948 -0.13600818 -0.20458192  0.02034034 -0.00940222\n",
      " -0.13363653  0.12899377 -0.3380589   0.06794979  0.09806946  0.16235379\n",
      "  0.01031238  0.02908066]\n"
     ]
    }
   ],
   "source": [
    "pca_sklearn = PCA(5, svd_solver='full')\n",
    "pca_sklearn.fit(X)\n",
    "print(pca_sklearn.components_[0])\n",
    "print(pca_sklearn.components_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.14144957, -0.14321122, -0.1395285 ])"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = np.array([-0.14144957, -0.14321122, -0.1395285])\n",
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14139642999999999"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(foo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all((abs(foo) > .139) & (abs(foo) < .145))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v1_top3_abs_mean = np.mean(np.abs(v1[:3])\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eta_0: 1e-12, t_0: 1e-12\n",
      "eta_0: 1e-12, t_0: 1e-11\n",
      "eta_0: 1e-12, t_0: 1e-10\n",
      "eta_0: 1e-12, t_0: 1e-09\n",
      "eta_0: 1e-12, t_0: 1e-08\n",
      "eta_0: 1e-12, t_0: 1e-07\n",
      "eta_0: 1e-12, t_0: 1e-06\n",
      "eta_0: 1e-12, t_0: 1e-05\n",
      "eta_0: 1e-12, t_0: 0.0001\n",
      "eta_0: 1e-12, t_0: 0.001\n",
      "eta_0: 1e-12, t_0: 0.01\n",
      "eta_0: 1e-12, t_0: 0.1\n",
      "eta_0: 1e-12, t_0: 1\n",
      "eta_0: 1e-12, t_0: 10\n",
      "eta_0: 1e-12, t_0: 100\n",
      "eta_0: 1e-12, t_0: 1000\n",
      "eta_0: 1e-12, t_0: 10000\n",
      "eta_0: 1e-11, t_0: 1e-12\n",
      "eta_0: 1e-11, t_0: 1e-11\n",
      "eta_0: 1e-11, t_0: 1e-10\n",
      "eta_0: 1e-11, t_0: 1e-09\n",
      "eta_0: 1e-11, t_0: 1e-08\n",
      "eta_0: 1e-11, t_0: 1e-07\n",
      "eta_0: 1e-11, t_0: 1e-06\n",
      "eta_0: 1e-11, t_0: 1e-05\n",
      "eta_0: 1e-11, t_0: 0.0001\n",
      "eta_0: 1e-11, t_0: 0.001\n",
      "eta_0: 1e-11, t_0: 0.01\n",
      "eta_0: 1e-11, t_0: 0.1\n",
      "eta_0: 1e-11, t_0: 1\n",
      "eta_0: 1e-11, t_0: 10\n",
      "eta_0: 1e-11, t_0: 100\n",
      "eta_0: 1e-11, t_0: 1000\n",
      "eta_0: 1e-11, t_0: 10000\n",
      "eta_0: 1e-10, t_0: 1e-12\n",
      "eta_0: 1e-10, t_0: 1e-11\n",
      "eta_0: 1e-10, t_0: 1e-10\n",
      "eta_0: 1e-10, t_0: 1e-09\n",
      "eta_0: 1e-10, t_0: 1e-08\n",
      "eta_0: 1e-10, t_0: 1e-07\n",
      "eta_0: 1e-10, t_0: 1e-06\n",
      "eta_0: 1e-10, t_0: 1e-05\n",
      "eta_0: 1e-10, t_0: 0.0001\n",
      "eta_0: 1e-10, t_0: 0.001\n",
      "eta_0: 1e-10, t_0: 0.01\n",
      "eta_0: 1e-10, t_0: 0.1\n",
      "eta_0: 1e-10, t_0: 1\n",
      "eta_0: 1e-10, t_0: 10\n",
      "eta_0: 1e-10, t_0: 100\n",
      "eta_0: 1e-10, t_0: 1000\n",
      "eta_0: 1e-10, t_0: 10000\n",
      "eta_0: 1e-09, t_0: 1e-12\n",
      "eta_0: 1e-09, t_0: 1e-11\n",
      "eta_0: 1e-09, t_0: 1e-10\n",
      "eta_0: 1e-09, t_0: 1e-09\n",
      "eta_0: 1e-09, t_0: 1e-08\n",
      "eta_0: 1e-09, t_0: 1e-07\n",
      "eta_0: 1e-09, t_0: 1e-06\n",
      "eta_0: 1e-09, t_0: 1e-05\n",
      "eta_0: 1e-09, t_0: 0.0001\n",
      "eta_0: 1e-09, t_0: 0.001\n",
      "eta_0: 1e-09, t_0: 0.01\n",
      "eta_0: 1e-09, t_0: 0.1\n",
      "eta_0: 1e-09, t_0: 1\n",
      "eta_0: 1e-09, t_0: 10\n",
      "eta_0: 1e-09, t_0: 100\n",
      "eta_0: 1e-09, t_0: 1000\n",
      "eta_0: 1e-09, t_0: 10000\n",
      "eta_0: 1e-08, t_0: 1e-12\n",
      "eta_0: 1e-08, t_0: 1e-11\n",
      "eta_0: 1e-08, t_0: 1e-10\n",
      "eta_0: 1e-08, t_0: 1e-09\n",
      "eta_0: 1e-08, t_0: 1e-08\n",
      "eta_0: 1e-08, t_0: 1e-07\n",
      "eta_0: 1e-08, t_0: 1e-06\n",
      "eta_0: 1e-08, t_0: 1e-05\n",
      "eta_0: 1e-08, t_0: 0.0001\n",
      "eta_0: 1e-08, t_0: 0.001\n",
      "eta_0: 1e-08, t_0: 0.01\n",
      "eta_0: 1e-08, t_0: 0.1\n",
      "eta_0: 1e-08, t_0: 1\n",
      "eta_0: 1e-08, t_0: 10\n",
      "eta_0: 1e-08, t_0: 100\n",
      "eta_0: 1e-08, t_0: 1000\n",
      "eta_0: 1e-08, t_0: 10000\n",
      "eta_0: 1e-07, t_0: 1e-12\n",
      "eta_0: 1e-07, t_0: 1e-11\n",
      "eta_0: 1e-07, t_0: 1e-10\n",
      "eta_0: 1e-07, t_0: 1e-09\n",
      "eta_0: 1e-07, t_0: 1e-08\n",
      "eta_0: 1e-07, t_0: 1e-07\n",
      "eta_0: 1e-07, t_0: 1e-06\n",
      "eta_0: 1e-07, t_0: 1e-05\n",
      "eta_0: 1e-07, t_0: 0.0001\n",
      "eta_0: 1e-07, t_0: 0.001\n",
      "eta_0: 1e-07, t_0: 0.01\n",
      "eta_0: 1e-07, t_0: 0.1\n",
      "eta_0: 1e-07, t_0: 1\n",
      "eta_0: 1e-07, t_0: 10\n",
      "eta_0: 1e-07, t_0: 100\n",
      "eta_0: 1e-07, t_0: 1000\n",
      "eta_0: 1e-07, t_0: 10000\n",
      "eta_0: 1e-06, t_0: 1e-12\n",
      "eta_0: 1e-06, t_0: 1e-11\n",
      "eta_0: 1e-06, t_0: 1e-10\n",
      "eta_0: 1e-06, t_0: 1e-09\n",
      "eta_0: 1e-06, t_0: 1e-08\n",
      "eta_0: 1e-06, t_0: 1e-07\n",
      "eta_0: 1e-06, t_0: 1e-06\n",
      "eta_0: 1e-06, t_0: 1e-05\n",
      "eta_0: 1e-06, t_0: 0.0001\n",
      "eta_0: 1e-06, t_0: 0.001\n",
      "eta_0: 1e-06, t_0: 0.01\n",
      "eta_0: 1e-06, t_0: 0.1\n",
      "eta_0: 1e-06, t_0: 1\n",
      "eta_0: 1e-06, t_0: 10\n",
      "eta_0: 1e-06, t_0: 100\n",
      "eta_0: 1e-06, t_0: 1000\n",
      "eta_0: 1e-06, t_0: 10000\n",
      "eta_0: 1e-05, t_0: 1e-12\n",
      "eta_0: 1e-05, t_0: 1e-11\n",
      "eta_0: 1e-05, t_0: 1e-10\n",
      "eta_0: 1e-05, t_0: 1e-09\n",
      "eta_0: 1e-05, t_0: 1e-08\n",
      "eta_0: 1e-05, t_0: 1e-07\n",
      "eta_0: 1e-05, t_0: 1e-06\n",
      "eta_0: 1e-05, t_0: 1e-05\n",
      "eta_0: 1e-05, t_0: 0.0001\n",
      "eta_0: 1e-05, t_0: 0.001\n",
      "eta_0: 1e-05, t_0: 0.01\n",
      "eta_0: 1e-05, t_0: 0.1\n",
      "eta_0: 1e-05, t_0: 1\n",
      "eta_0: 1e-05, t_0: 10\n",
      "eta_0: 1e-05, t_0: 100\n",
      "eta_0: 1e-05, t_0: 1000\n",
      "eta_0: 1e-05, t_0: 10000\n",
      "eta_0: 0.0001, t_0: 1e-12\n",
      "[ 0.14116643  0.14179079  0.14263673] - [-0.02568945 -0.16781164 -0.12617525]\n",
      "eta_0: 0.0001, t_0: 1e-11\n",
      "[ 0.14260621  0.14360535  0.14163803] - [ 0.25157902 -0.08537787  0.10793173]\n",
      "[ 0.14276384  0.14388862  0.14186274] - [-0.02388112  0.02468034  0.07992172]\n",
      "[-0.14313333 -0.14234492 -0.14077626] - [-0.08338585 -0.03569922 -0.07226859]\n",
      "eta_0: 0.0001, t_0: 1e-10\n",
      "[ 0.14138506  0.14251657  0.14186079] - [ 0.0206296   0.30793975  0.04215437]\n",
      "eta_0: 0.0001, t_0: 1e-09\n",
      "[-0.14196609 -0.14304808 -0.14099188] - [-0.01429151 -0.13102321  0.11871535]\n",
      "[ 0.14174064  0.14384279  0.14052174] - [ 0.03435442  0.01603535  0.08582506]\n",
      "eta_0: 0.0001, t_0: 1e-08\n",
      "[-0.14276069 -0.14356412 -0.14163881] - [ 0.11486802 -0.10506577 -0.10334232]\n",
      "[ 0.1422505   0.14417279  0.14012396] - [ 0.04637936  0.083325   -0.28735741]\n",
      "eta_0: 0.0001, t_0: 1e-07\n",
      "[-0.14173089 -0.14336702 -0.14137852] - [-0.15332016  0.11767619  0.29559219]\n",
      "[ 0.14215458  0.14305473  0.14108566] - [-0.14408932  0.17470391  0.07517502]\n",
      "[-0.14188235 -0.14348375 -0.14087355] - [-0.19482277 -0.07982338  0.01389822]\n",
      "eta_0: 0.0001, t_0: 1e-06\n",
      "[-0.14151443 -0.14205923 -0.14238276] - [ 0.15667779  0.18945051  0.02171691]\n",
      "eta_0: 0.0001, t_0: 1e-05\n",
      "[ 0.14174203  0.14300232  0.14076174] - [ 0.11022788 -0.13740859  0.11994423]\n",
      "[ 0.14183723  0.14385737  0.14075717] - [-0.31447564  0.03108766 -0.08515006]\n",
      "[-0.14238121 -0.14235868 -0.14044067] - [-0.16899639 -0.19628563 -0.00151245]\n",
      "eta_0: 0.0001, t_0: 0.0001\n",
      "[ 0.14160024  0.14405961  0.14118322] - [-0.0117232  -0.0201427  -0.12802221]\n",
      "eta_0: 0.0001, t_0: 0.001\n",
      "[-0.1417141  -0.1433631  -0.14092162] - [ 0.20575449  0.0143732   0.16008708]\n",
      "[ 0.14317397  0.14260454  0.14111382] - [-0.1053028   0.14370091  0.0203002 ]\n",
      "eta_0: 0.0001, t_0: 0.01\n",
      "[ 0.14162525  0.14498618  0.14209221] - [-0.10537669  0.20415068 -0.08282379]\n",
      "eta_0: 0.0001, t_0: 0.1\n",
      "eta_0: 0.0001, t_0: 1\n",
      "eta_0: 0.0001, t_0: 10\n",
      "eta_0: 0.0001, t_0: 100\n",
      "eta_0: 0.0001, t_0: 1000\n",
      "eta_0: 0.0001, t_0: 10000\n",
      "eta_0: 0.001, t_0: 1e-12\n",
      "[ 0.1423337   0.14326067  0.1412034 ] - [ 0.03896457  0.18371909  0.10978296]\n",
      "[ 0.14233389  0.14325895  0.14120405] - [-0.06479269  0.07222736  0.09717455]\n",
      "[ 0.14233394  0.14325929  0.14120448] - [-0.16606912 -0.2077173  -0.010343  ]\n",
      "eta_0: 0.001, t_0: 1e-11\n",
      "[-0.14233294 -0.14325927 -0.14120365] - [-0.06124487 -0.07182587  0.04385191]\n",
      "[-0.14233356 -0.14325957 -0.1412045 ] - [ 0.04695916  0.08410989 -0.28585029]\n",
      "[-0.14233424 -0.14325878 -0.14120374] - [-0.144253   -0.24233194 -0.16320088]\n",
      "eta_0: 0.001, t_0: 1e-10\n",
      "[ 0.14233379  0.14325885  0.14120425] - [-0.00958087 -0.12491446 -0.07603853]\n",
      "[-0.14233324 -0.14326005 -0.14120362] - [ 0.14452263 -0.17641617 -0.07618004]\n",
      "[-0.14233289 -0.14325879 -0.14120319] - [ 0.16471169  0.05626889  0.0721411 ]\n",
      "eta_0: 0.001, t_0: 1e-09\n",
      "[-0.14233306 -0.14326048 -0.14120502] - [ 0.02836504 -0.03812272  0.18403885]\n",
      "[ 0.14233441  0.14326073  0.14120288] - [ 0.26987762 -0.13637737  0.14000501]\n",
      "[ 0.14233325  0.14325961  0.14120336] - [ 0.04731937  0.08270187 -0.2866205 ]\n",
      "eta_0: 0.001, t_0: 1e-08\n",
      "[-0.14233443 -0.14325981 -0.14120436] - [-0.11073829 -0.28013945 -0.06280891]\n",
      "[-0.14233321 -0.14325918 -0.14120369] - [ 0.15148267 -0.20271188 -0.1066877 ]\n",
      "[ 0.14233382  0.1432589   0.14120388] - [-0.11307031  0.10706163  0.10331177]\n",
      "eta_0: 0.001, t_0: 1e-07\n",
      "[ 0.14233405  0.14325953  0.14120349] - [ 0.11397513 -0.10513117 -0.1038204 ]\n",
      "[ 0.1423336   0.14325959  0.1412049 ] - [ 0.03552092  0.0188963   0.08618826]\n",
      "[ 0.14233376  0.14325937  0.14120319] - [-0.06978248  0.08009687 -0.01225306]\n",
      "eta_0: 0.001, t_0: 1e-06\n",
      "[-0.14233338 -0.14325962 -0.14120434] - [-0.08457072  0.03563792  0.0925518 ]\n",
      "[-0.14233403 -0.14326048 -0.14120393] - [-0.39494051 -0.29419381  0.03322059]\n",
      "[ 0.14233368  0.14325978  0.14120463] - [ 0.20164099  0.3286856  -0.16784212]\n",
      "eta_0: 0.001, t_0: 1e-05\n",
      "[ 0.1423355   0.14326068  0.14120395] - [ 0.03818046 -0.17639417  0.07206971]\n",
      "[-0.14233483 -0.14325988 -0.14120468] - [ 0.04615554 -0.15014635  0.10303036]\n",
      "[ 0.14233463  0.14325988  0.14120483] - [ 0.0134741   0.30193523  0.04913494]\n",
      "eta_0: 0.001, t_0: 0.0001\n",
      "[-0.14233324 -0.14325931 -0.14120348] - [ 0.00961581 -0.18135119  0.19177063]\n",
      "[ 0.14233364  0.14325903  0.14120392] - [-0.02331662 -0.14155244  0.11905395]\n",
      "[ 0.1423342   0.14326022  0.14120329] - [ 0.39130152  0.30029412 -0.03105731]\n",
      "eta_0: 0.001, t_0: 0.001\n",
      "[-0.14233348 -0.14325961 -0.14120333] - [-0.15451686  0.20594467  0.09187906]\n",
      "[ 0.14233364  0.1432598   0.14120439] - [ 0.17646817  0.12140607  0.09253911]\n",
      "[-0.14233456 -0.14325886 -0.14120302] - [-0.32805169  0.02586919  0.03089909]\n",
      "eta_0: 0.001, t_0: 0.01\n",
      "[ 0.14233498  0.14325987  0.14120529] - [ 0.1252763  -0.2152943   0.00865342]\n",
      "[ 0.14233384  0.14325982  0.14120431] - [ 0.01344827 -0.07877426  0.09278998]\n",
      "[ 0.1423343   0.1432596   0.14120314] - [ 0.24110659 -0.01717418 -0.08355344]\n",
      "eta_0: 0.001, t_0: 0.1\n",
      "[-0.14233295 -0.14326012 -0.14120335] - [-0.18592323  0.17474438  0.02746241]\n",
      "[-0.14233509 -0.14326047 -0.14120426] - [ 0.01025771  0.01693435  0.10302286]\n",
      "[ 0.14233432  0.14326004  0.14120498] - [-0.05344436  0.08780885 -0.02738868]\n",
      "eta_0: 0.001, t_0: 1\n",
      "[-0.14233385 -0.14326093 -0.14120368] - [ 0.3598443  -0.14169694 -0.05329629]\n",
      "[ 0.142335    0.14326091  0.14120473] - [-0.07160133  0.16943541  0.20258787]\n",
      "[-0.14233504 -0.143261   -0.14120367] - [ 0.06376259  0.14493248  0.06178492]\n",
      "eta_0: 0.001, t_0: 10\n",
      "[ 0.14233324  0.14325834  0.14120418] - [-0.09710059 -0.23341077 -0.15849241]\n",
      "[ 0.14233379  0.14325894  0.14120464] - [ 0.20977313  0.24841614 -0.17883788]\n",
      "[-0.14233463 -0.14326051 -0.1412038 ] - [ 0.05434577  0.07024554  0.11261771]\n",
      "eta_0: 0.001, t_0: 100\n",
      "[-0.1423351  -0.14326094 -0.14120392] - [ 0.10491058 -0.10086388 -0.08130609]\n",
      "[-0.1423341  -0.14326019 -0.14120442] - [-0.07841929  0.07895825 -0.28988124]\n",
      "[-0.14233412 -0.14325913 -0.14120305] - [-0.06072556 -0.06169407  0.14750045]\n",
      "eta_0: 0.001, t_0: 1000\n",
      "[-0.14213532 -0.14307847 -0.14124235] - [ 0.16401738  0.14833377 -0.04568783]\n",
      "[ 0.14236099  0.14329482  0.14106122] - [ 0.03322261  0.0396166  -0.10351216]\n",
      "[-0.14235831 -0.14323205 -0.14128045] - [-0.05457785  0.02046158 -0.12763463]\n",
      "eta_0: 0.001, t_0: 10000\n",
      "eta_0: 0.01, t_0: 1e-12\n",
      "[ 0.14232016  0.1432411   0.14119808] - [-0.03348417 -0.03310551 -0.09975684]\n",
      "[-0.14235632 -0.14327736 -0.14122074] - [-0.14149705 -0.1222571   0.13662918]\n",
      "[ 0.14234203  0.14324665  0.14119732] - [-0.12308899 -0.12478391 -0.13941732]\n",
      "eta_0: 0.01, t_0: 1e-11\n",
      "[ 0.1423107   0.14328836  0.14121556] - [ 0.00276653  0.0424651   0.24970111]\n",
      "[ 0.1423363   0.14322577  0.14120785] - [ 0.29118834  0.1761544  -0.04973536]\n",
      "[ 0.14234366  0.14325538  0.14119996] - [-0.00889909 -0.03357398 -0.26006901]\n",
      "eta_0: 0.01, t_0: 1e-10\n",
      "[-0.14232881 -0.14325395 -0.14118485] - [ 0.03156139 -0.08948837  0.02948558]\n",
      "[ 0.1423479   0.14327947  0.14118453] - [ 0.20974836  0.27556203 -0.27391434]\n",
      "[-0.1423053  -0.14322858 -0.14122897] - [-0.2736598   0.13357914 -0.13955061]\n",
      "eta_0: 0.01, t_0: 1e-09\n",
      "[ 0.14233359  0.14324441  0.14119553] - [ 0.19391479  0.07735417  0.16505708]\n",
      "[ 0.14232614  0.14322531  0.14121108] - [ 0.21133797  0.32654961 -0.16131523]\n",
      "[ 0.14231927  0.1432924   0.14121023] - [-0.13738528 -0.11404034 -0.14030466]\n",
      "eta_0: 0.01, t_0: 1e-08\n",
      "[ 0.14230414  0.14327013  0.14117552] - [-0.14779009 -0.13798676  0.14253117]\n",
      "[-0.14233057 -0.14327676 -0.14119446] - [-0.12433673  0.09924826  0.01836733]\n",
      "[-0.14230858 -0.14330906 -0.14123241] - [-0.19195825 -0.10277372  0.00369129]\n",
      "eta_0: 0.01, t_0: 1e-07\n",
      "[ 0.14233077  0.14325318  0.14117529] - [ 0.1915943   0.08913379 -0.04093055]\n",
      "[ 0.14234532  0.1432529   0.14123135] - [-0.04517958  0.16019854 -0.11940959]\n",
      "[-0.14236592 -0.14328571 -0.14121012] - [ 0.05225016 -0.08843278  0.13022103]\n",
      "eta_0: 0.01, t_0: 1e-06\n",
      "[ 0.14235234  0.14326016  0.14119011] - [-0.08061456  0.12327975  0.00417081]\n",
      "[-0.14232486 -0.14322978 -0.14122783] - [-0.02375475  0.15401612 -0.11928919]\n",
      "[-0.14235394 -0.14324841 -0.14121485] - [ 0.15107328 -0.20059779 -0.09861647]\n",
      "eta_0: 0.01, t_0: 1e-05\n",
      "[ 0.14234907  0.14322708  0.14116698] - [ 0.3152875  -0.01246934 -0.05153115]\n",
      "[ 0.1423428   0.14324801  0.14120512] - [ 0.165595    0.08712934  0.05187938]\n",
      "[ 0.14234956  0.14331021  0.14119435] - [ 0.04381825 -0.18254485  0.0637428 ]\n",
      "eta_0: 0.01, t_0: 0.0001\n",
      "[-0.14237918 -0.14323257 -0.14118664] - [ 0.07190475  0.08855044 -0.05947268]\n",
      "[-0.1423436  -0.143249   -0.14117408] - [ 0.18709172  0.10022904 -0.05055166]\n",
      "[ 0.1423146   0.14325269  0.14120652] - [-0.04234269  0.06891157 -0.16386167]\n",
      "eta_0: 0.01, t_0: 0.001\n",
      "[ 0.14230838  0.14326347  0.14121001] - [ 0.10561307 -0.48050706  0.28970702]\n",
      "[ 0.14233621  0.14324963  0.14122204] - [-0.00790764  0.01697462  0.07349418]\n",
      "[ 0.14232444  0.14324465  0.1411805 ] - [-0.14428589  0.11988768  0.28612307]\n",
      "eta_0: 0.01, t_0: 0.01\n",
      "[-0.14231866 -0.14326458 -0.14122192] - [-0.12664157  0.11006575 -0.10463441]\n",
      "[-0.1423294  -0.14326484 -0.14118207] - [ 0.05958349  0.09797678 -0.24355586]\n",
      "[-0.14233862 -0.14328297 -0.14119319] - [-0.15600225 -0.03934888 -0.24305596]\n",
      "eta_0: 0.01, t_0: 0.1\n",
      "[ 0.14232235  0.1432456   0.14121215] - [-0.16091482  0.20388571 -0.08474328]\n",
      "[ 0.14231326  0.14321395  0.14121137] - [-0.09481718  0.18488419  0.02359936]\n",
      "[-0.14235595 -0.14328538 -0.14121407] - [-0.21308661 -0.06434586 -0.28746294]\n",
      "eta_0: 0.01, t_0: 1\n",
      "[-0.14238767 -0.14323523 -0.14118198] - [ 0.07727826 -0.15468573  0.02189911]\n",
      "[-0.14236842 -0.14328317 -0.14119349] - [-0.15438985  0.0065741  -0.11027818]\n",
      "[ 0.14233883  0.14326102  0.14120365] - [-0.00609818 -0.03930886  0.08724968]\n",
      "eta_0: 0.01, t_0: 10\n",
      "[-0.14231391 -0.14327607 -0.14120539] - [-0.16626235  0.14133183 -0.28566891]\n",
      "[-0.14236016 -0.14329122 -0.14122238] - [ 0.00608652 -0.05948053  0.00551542]\n",
      "[-0.14234116 -0.14324913 -0.1412097 ] - [-0.08453396 -0.04645605  0.0590047 ]\n",
      "eta_0: 0.01, t_0: 100\n",
      "[-0.14233307 -0.14331914 -0.14120575] - [ 0.3695122  -0.05026025  0.15500565]\n",
      "[ 0.14234102  0.14325967  0.14122634] - [-0.04669327  0.15722744 -0.06098507]\n",
      "[-0.14234932 -0.14330987 -0.14116903] - [-0.29884925 -0.11569702  0.20489394]\n",
      "eta_0: 0.01, t_0: 1000\n",
      "[-0.14235033 -0.14323992 -0.14120044] - [ 0.03156862 -0.19961603 -0.00824027]\n",
      "[ 0.14239145  0.14328245  0.14119073] - [ 0.06400564  0.11661256  0.06979076]\n",
      "[-0.14231945 -0.14327381 -0.1411953 ] - [-0.19154661  0.00585533 -0.11656904]\n",
      "eta_0: 0.01, t_0: 10000\n",
      "[ 0.14232636  0.14327836  0.14122134] - [-0.10746142 -0.07086192 -0.00916428]\n",
      "[ 0.14234464  0.14326426  0.14121273] - [ 0.38135143  0.01779035  0.05684228]\n",
      "[ 0.14231807  0.14324441  0.14120067] - [ 0.0246815  -0.05104621  0.14148891]\n",
      "eta_0: 0.1, t_0: 1e-12\n",
      "[ 0.14327609  0.1432397   0.14116408] - [ 0.0916646   0.16099075 -0.05203462]\n",
      "[-0.14282304 -0.14366034 -0.14144939] - [ 0.33325938 -0.1899335   0.2645336 ]\n",
      "[-0.14132906 -0.14388079 -0.14172427] - [ 0.21779585  0.18413289 -0.01230665]\n",
      "eta_0: 0.1, t_0: 1e-11\n",
      "[ 0.14242887  0.14226167  0.14075613] - [-0.14692719 -0.35030134  0.13146807]\n",
      "[ 0.14256477  0.14479446  0.14063998] - [ 0.01468524 -0.07564129 -0.09628045]\n",
      "[ 0.1415786   0.14274194  0.14005288] - [-0.04024987  0.04161867 -0.16376965]\n",
      "eta_0: 0.1, t_0: 1e-10\n",
      "[ 0.14151758  0.14465429  0.14088044] - [ 0.12256152  0.347785   -0.20775214]\n",
      "[-0.14174843 -0.14341362 -0.140437  ] - [-0.04934604  0.16718279 -0.21581118]\n",
      "[-0.14212366 -0.14328034 -0.14094673] - [-0.1317345   0.00205942 -0.09888009]\n",
      "eta_0: 0.1, t_0: 1e-09\n",
      "[ 0.14290454  0.14291531  0.14148966] - [ 0.0840173   0.16781119 -0.25925902]\n",
      "[ 0.14244408  0.1429187   0.14105234] - [ 0.00968869  0.00591167  0.02556483]\n",
      "[ 0.14241812  0.14335585  0.14164835] - [-0.01042108 -0.10695354  0.23737772]\n",
      "eta_0: 0.1, t_0: 1e-08\n",
      "[ 0.14234083  0.14321449  0.14109776] - [-0.24768139 -0.1245026  -0.07682404]\n",
      "[ 0.14230998  0.14359618  0.14174423] - [-0.0467584 -0.1448046 -0.0512755]\n",
      "[-0.14153923 -0.14327032 -0.14136782] - [-0.04402524 -0.24253544 -0.06495892]\n",
      "eta_0: 0.1, t_0: 1e-07\n",
      "[ 0.14272993  0.14257546  0.14163905] - [ 0.06728522  0.04133487  0.08690204]\n",
      "[-0.14228247 -0.1437891  -0.14196415] - [ 0.04808532  0.17963828 -0.25406223]\n",
      "[ 0.1422866   0.14382776  0.14021313] - [-0.01064483  0.00166803 -0.01722486]\n",
      "eta_0: 0.1, t_0: 1e-06\n",
      "[-0.14300083 -0.14363109 -0.14136245] - [ 0.1493304   0.12280246 -0.06309892]\n",
      "[-0.14254088 -0.14380958 -0.14190605] - [ 0.21635262  0.17810714 -0.03416528]\n",
      "[-0.1420166  -0.14346678 -0.14100554] - [-0.06594322  0.13506057 -0.201811  ]\n",
      "eta_0: 0.1, t_0: 1e-05\n",
      "[-0.14295288 -0.14254302 -0.14133916] - [ 0.05504963  0.3037647  -0.15511627]\n",
      "[-0.14216878 -0.14337847 -0.14071117] - [ 0.02907182  0.15081676  0.08827703]\n",
      "[ 0.14212189  0.14202103  0.14063879] - [-0.07696113 -0.11171416 -0.16463816]\n",
      "eta_0: 0.1, t_0: 0.0001\n",
      "[ 0.14218948  0.14311869  0.14100094] - [-0.16666269  0.19659218  0.13767827]\n",
      "[-0.14156256 -0.14363151 -0.14068145] - [-0.02059261 -0.07431988 -0.03237821]\n",
      "[ 0.14228533  0.14314482  0.14131705] - [ 0.07094495  0.00555414 -0.08871585]\n",
      "eta_0: 0.1, t_0: 0.001\n",
      "[-0.14200645 -0.14275237 -0.14070403] - [-0.18262708 -0.24169976 -0.07909244]\n",
      "[-0.14200848 -0.14286374 -0.14108806] - [ 0.13453141 -0.11174209  0.21682597]\n",
      "[-0.14288381 -0.14400117 -0.14049875] - [-0.27673109 -0.03628499  0.07470515]\n",
      "eta_0: 0.1, t_0: 0.01\n",
      "[ 0.14210602  0.14363498  0.14263397] - [ 0.06655431 -0.20671567  0.22264525]\n",
      "[ 0.1424115   0.14349538  0.14063035] - [ 0.04201173  0.30114924  0.04209624]\n",
      "[ 0.14283036  0.14454561  0.14094069] - [ 0.39643384 -0.10757732  0.10917909]\n",
      "eta_0: 0.1, t_0: 0.1\n",
      "[-0.1429127  -0.14365496 -0.1415308 ] - [-0.24127342 -0.14022689  0.03421178]\n",
      "[ 0.14188223  0.14321455  0.14083964] - [ 0.01120273 -0.1091509   0.0659781 ]\n",
      "[-0.14327687 -0.14339083 -0.14078352] - [ 0.12400839  0.09626446 -0.06562056]\n",
      "eta_0: 0.1, t_0: 1\n",
      "[-0.14273995 -0.14335203 -0.14064726] - [ 0.14785779  0.26549558 -0.22886512]\n",
      "[ 0.14243338  0.14416354  0.14172742] - [-0.15490508  0.22012895 -0.11786027]\n",
      "[-0.14253819 -0.14375477 -0.14099936] - [ 0.10755141  0.10972528 -0.11248609]\n",
      "eta_0: 0.1, t_0: 10\n",
      "[-0.14267135 -0.14399207 -0.14066489] - [ 0.07863096 -0.0792633   0.12966847]\n",
      "[-0.14328199 -0.14329996 -0.14146895] - [-0.0221888   0.01515823 -0.18242247]\n",
      "[ 0.14267352  0.1438736   0.14214437] - [-0.12439651  0.11711389 -0.20019231]\n",
      "eta_0: 0.1, t_0: 100\n",
      "[ 0.1423471   0.14273256  0.14162364] - [ 0.21624103  0.1413483  -0.1768214 ]\n",
      "[ 0.14116943  0.14287595  0.14112571] - [ 0.20394724  0.01827749  0.00045566]\n",
      "[-0.14209145 -0.14252808 -0.14147554] - [-0.2052459  -0.18861288  0.09477184]\n",
      "eta_0: 0.1, t_0: 1000\n",
      "[-0.14174073 -0.14343667 -0.14086984] - [ 0.16076881 -0.06192442  0.18677824]\n",
      "[ 0.14258806  0.14215645  0.14153048] - [ 0.17451838  0.37827721  0.25322697]\n",
      "[-0.14225332 -0.14308324 -0.14148389] - [-0.09303165 -0.00552723 -0.02279031]\n",
      "eta_0: 0.1, t_0: 10000\n",
      "[-0.14249379 -0.14378943 -0.14133564] - [ 0.00971127 -0.13124346 -0.1561394 ]\n",
      "[ 0.14222322  0.14319945  0.14076795] - [-0.02929329  0.00230429 -0.01210389]\n",
      "[ 0.14223199  0.14370983  0.14120071] - [ 0.03156848  0.15997125  0.19987384]\n",
      "eta_0: 1, t_0: 1e-12\n",
      "eta_0: 1, t_0: 1e-11\n",
      "eta_0: 1, t_0: 1e-10\n",
      "eta_0: 1, t_0: 1e-09\n",
      "eta_0: 1, t_0: 1e-08\n",
      "eta_0: 1, t_0: 1e-07\n",
      "eta_0: 1, t_0: 1e-06\n",
      "eta_0: 1, t_0: 1e-05\n",
      "eta_0: 1, t_0: 0.0001\n",
      "eta_0: 1, t_0: 0.001\n",
      "[-0.14402537 -0.13955344 -0.14200703] - [-0.06471562  0.12677522 -0.04478126]\n",
      "[ 0.1423611   0.14450642  0.14376611] - [-0.03910552  0.05283921 -0.17244842]\n",
      "eta_0: 1, t_0: 0.01\n",
      "eta_0: 1, t_0: 0.1\n",
      "[ 0.14087499  0.14189489  0.13960671] - [-0.07819467 -0.08229536 -0.1246308 ]\n",
      "eta_0: 1, t_0: 1\n",
      "[-0.14385148 -0.14344256 -0.14329934] - [ 0.00699567  0.00144563  0.14798514]\n",
      "eta_0: 1, t_0: 10\n",
      "eta_0: 1, t_0: 100\n",
      "eta_0: 1, t_0: 1000\n",
      "eta_0: 1, t_0: 10000\n",
      "[-0.13988592 -0.14393974 -0.14038884] - [ 0.09397321  0.20791288 -0.10123012]\n",
      "eta_0: 10, t_0: 1e-12\n",
      "eta_0: 10, t_0: 1e-11\n",
      "eta_0: 10, t_0: 1e-10\n",
      "eta_0: 10, t_0: 1e-09\n",
      "eta_0: 10, t_0: 1e-08\n",
      "eta_0: 10, t_0: 1e-07\n",
      "eta_0: 10, t_0: 1e-06\n",
      "eta_0: 10, t_0: 1e-05\n",
      "eta_0: 10, t_0: 0.0001\n",
      "eta_0: 10, t_0: 0.001\n",
      "eta_0: 10, t_0: 0.01\n",
      "eta_0: 10, t_0: 0.1\n",
      "eta_0: 10, t_0: 1\n",
      "eta_0: 10, t_0: 10\n",
      "eta_0: 10, t_0: 100\n",
      "[-0.13917729 -0.14495511 -0.13903424] - [-0.04171952  0.02086776 -0.05726896]\n",
      "eta_0: 10, t_0: 1000\n",
      "eta_0: 10, t_0: 10000\n",
      "eta_0: 100, t_0: 1e-12\n",
      "eta_0: 100, t_0: 1e-11\n",
      "eta_0: 100, t_0: 1e-10\n",
      "eta_0: 100, t_0: 1e-09\n",
      "eta_0: 100, t_0: 1e-08\n",
      "eta_0: 100, t_0: 1e-07\n",
      "eta_0: 100, t_0: 1e-06\n",
      "eta_0: 100, t_0: 1e-05\n",
      "eta_0: 100, t_0: 0.0001\n",
      "eta_0: 100, t_0: 0.001\n",
      "eta_0: 100, t_0: 0.01\n",
      "eta_0: 100, t_0: 0.1\n",
      "eta_0: 100, t_0: 1\n",
      "eta_0: 100, t_0: 10\n",
      "eta_0: 100, t_0: 100\n",
      "eta_0: 100, t_0: 1000\n",
      "eta_0: 100, t_0: 10000\n",
      "eta_0: 1000, t_0: 1e-12\n",
      "eta_0: 1000, t_0: 1e-11\n",
      "eta_0: 1000, t_0: 1e-10\n",
      "eta_0: 1000, t_0: 1e-09\n",
      "eta_0: 1000, t_0: 1e-08\n",
      "eta_0: 1000, t_0: 1e-07\n",
      "eta_0: 1000, t_0: 1e-06\n",
      "eta_0: 1000, t_0: 1e-05\n",
      "eta_0: 1000, t_0: 0.0001\n",
      "eta_0: 1000, t_0: 0.001\n",
      "eta_0: 1000, t_0: 0.01\n",
      "eta_0: 1000, t_0: 0.1\n",
      "eta_0: 1000, t_0: 1\n",
      "eta_0: 1000, t_0: 10\n",
      "eta_0: 1000, t_0: 100\n",
      "eta_0: 1000, t_0: 1000\n",
      "eta_0: 1000, t_0: 10000\n",
      "eta_0: 10000, t_0: 1e-12\n",
      "eta_0: 10000, t_0: 1e-11\n",
      "eta_0: 10000, t_0: 1e-10\n",
      "eta_0: 10000, t_0: 1e-09\n",
      "eta_0: 10000, t_0: 1e-08\n",
      "eta_0: 10000, t_0: 1e-07\n",
      "eta_0: 10000, t_0: 1e-06\n",
      "eta_0: 10000, t_0: 1e-05\n",
      "eta_0: 10000, t_0: 0.0001\n",
      "eta_0: 10000, t_0: 0.001\n",
      "eta_0: 10000, t_0: 0.01\n",
      "eta_0: 10000, t_0: 0.1\n",
      "eta_0: 10000, t_0: 1\n",
      "eta_0: 10000, t_0: 10\n",
      "eta_0: 10000, t_0: 100\n",
      "eta_0: 10000, t_0: 1000\n",
      "eta_0: 10000, t_0: 10000\n"
     ]
    }
   ],
   "source": [
    "#eta_0, t_0, epoch_count = .001, 1, 250\n",
    "Z = X - np.mean(X, axis=0)  # center the data\n",
    "\n",
    "epoch_count = 250\n",
    "low_thresh = 0.139\n",
    "high_thresh = 0.145\n",
    "\n",
    "for eta_0 in [10**k for k in range(-12, 5)]:\n",
    "    for t_0 in [10**k for k in range(-12, 5)]:\n",
    "        print(\"eta_0: {}, t_0: {}\".format(eta_0, t_0))\n",
    "        for i in range(0,3):\n",
    "            a_0 = np.random.randn(np.size(Z, 1))  # starting point\n",
    "            a_0 /= np.linalg.norm(a_0, axis=0)\n",
    "            v1, _ = pca.oja(copy.deepcopy(Z), a_0, eta_0, t_0, epoch_count)\n",
    "            #print(v1[:3])\n",
    "\n",
    "            Z1 = pca.deflate(Z, v1)\n",
    "            v2, _ = pca.oja(copy.deepcopy(Z1), a_0, eta_0, t_0, epoch_count)\n",
    "            #print(v2[:3])\n",
    "\n",
    "            #v1_top3_abs_mean = np.mean(np.abs(v1[:3]))\n",
    "            if np.all((np.abs(v1[:3]) > low_thresh) & (np.abs(v1[:3]) < high_thresh)):\n",
    "            #if np.all((np.abs(v1[:3]) > low_thresh) & (np.abs(v1[:3]) < high_thresh))\n",
    "            #if (0.139 < v1_top3_abs_mean) and (v1_top3_abs_mean < 0.145):\n",
    "                print(\"{} - {}\".format(v1[:3], v2[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=50, random_state=None,\n",
       "  svd_solver='randomized', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_sklearn = PCA(50, svd_solver='randomized')\n",
    "pca_sklearn.fit(d_values_centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.14820248, -0.15577997, -0.02747821,  0.05217483,  0.09001604,\n",
       "        0.21697314,  0.04838229,  0.0579049 ,  0.1868853 ,  0.07290217,\n",
       "       -0.1472483 ,  0.10724179, -0.21673595, -0.23128117,  0.08330134,\n",
       "        0.00877515,  0.22128086,  0.03809422,  0.00873939, -0.0690226 ,\n",
       "        0.24575393,  0.15597047,  0.08329307, -0.00890795, -0.25341302,\n",
       "        0.06472497, -0.00552899,  0.02767339, -0.23068174, -0.2512671 ,\n",
       "        0.02564334,  0.17065277,  0.19201366, -0.20402114,  0.12081652,\n",
       "       -0.03829716,  0.02052802, -0.00479948, -0.13600818, -0.20458192,\n",
       "        0.02034034, -0.00940222, -0.13363653,  0.12899377, -0.3380589 ,\n",
       "        0.06794979,  0.09806946,  0.16235379,  0.01031238,  0.02908066])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_sklearn.components_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Three"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*\"(a) Use the numpy.random.normal() function to generate a predictor X of length n = 100, as well as a noise vector ε of length n = 100.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100,), (100,))"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.normal(size=100)\n",
    "e = np.random.normal(size=100)\n",
    "(X.shape, e.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*\"(b) Generate a response vector Y of length n = 100 according to the model Y =β0 +β1X+β2X2 +β3X3 +ε,\n",
    "where β0, β1, β2, and β3 are constants of your choice. Here superscripts denote powers of X.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm just choosing some coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b0 = 27\n",
    "b1 = 11\n",
    "b2 = 5\n",
    "b3 = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = b0 + (b1*X) + (b2*(X**2)) + (b3*(X**3)) + e\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and we'll make X a 100,1 array rather than a vector,\n",
    "# for better use w/ sklearn functions\n",
    "X = X[:, np.newaxis]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll scale X and center y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_scaled = preprocessing.scale(X)\n",
    "y_centered = preprocessing.scale(y, with_std=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*\"(c) Now fit a LASSO model to the simulated data using your own coordinate descent algorithm. Use X, X2, . . . , X10 as predictors. Set λ = 1.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I found with lambda = 1 that I didn't get close convergence for all coefficients: although some of the higher coefficients had numbers didn't jump around a huge amount, they did jump around some. That said, the lower coefficients - the first four - stayed pretty constant (and the fifth and six were zeroed out by the algorithm). (This held regardless of whether I bumped up the iterations to something like 5000.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled_poly10 = PolynomialFeatures(degree=10, include_bias=False).fit_transform(X_scaled)\n",
    "X_scaled_poly10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = np.size(X_scaled_poly10, 0)\n",
    "beta_init = np.zeros(np.size(X_scaled_poly10, 1))\n",
    "max_iter = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.10616632e+01,  -4.47876250e+00,   1.85917711e+00,\n",
       "         3.58648303e+00,   2.25145468e-01,  -2.23217761e-01,\n",
       "         1.10173283e-01,  -5.57980881e-02,  -2.43346606e-02,\n",
       "         3.29610103e-03])"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas_rand = cd.randcoorddescent(beta_init, X_scaled_poly10, y_centered, 1, max_iter)\n",
    "betas_rand[-1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*\"(d) Select the optimal value of λ on a held-out validation set using your own coordinate descent algorithm. Create plots of the error on the validation set as a function of λ for your own coordinate descent algorithm and scikit-learn’s LASSO algorithm; use different colors for the two curves. Discuss the results obtained.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** check logic - am I thinking about it correctly and am I using the right X and y arrays in the right places?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((75, 10), (25, 10), (75,), (25,))"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled_poly10, y_centered, test_size=0.25, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mse_from_beta_coefs(beta_coefs, X, y):\n",
    "    return mean_squared_error(y, X.dot(beta_coefs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1e-08, 1e-07, 1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas = [10**k for k in range(-8, 5)]\n",
    "lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1e-08, 15.032272951315663),\n",
       " (1e-07, 14.773063823904012),\n",
       " (1e-06, 13.831467840913424),\n",
       " (1e-05, 14.819318488287136),\n",
       " (0.0001, 12.685048854144172),\n",
       " (0.001, 12.759550988787593),\n",
       " (0.01, 16.017532813899734),\n",
       " (0.1, 15.215005983449151),\n",
       " (1, 14.703783691335966),\n",
       " (10, 12.397645417104584),\n",
       " (100, 14.161035327909318),\n",
       " (1000, 55.173252618094494),\n",
       " (10000, 71.799899794378305)]"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mse_for_lambda_myimpl(lam, X_train, X_test, y_train, y_test):\n",
    "    beta_init = np.zeros(np.size(X_train, 1))\n",
    "    betas_rand = cd.randcoorddescent(beta_init, X_train, y_train, lam, max_iter)\n",
    "    return(mse_from_beta_coefs(betas_rand[-1, :], X_test, y_test))\n",
    "\n",
    "mses_myimpl = [mse_for_lambda_myimpl(lam, X_train, X_test, y_train, y_test) for lam in lambdas]\n",
    "list(zip(lambdas, mses_myimpl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1e-08, 25.357573773862814),\n",
       " (1e-07, 25.357483378345918),\n",
       " (1e-06, 25.356579469731113),\n",
       " (1e-05, 25.347542907902103),\n",
       " (0.0001, 25.257431056542188),\n",
       " (0.001, 24.380343700298663),\n",
       " (0.01, 14.871338640582312),\n",
       " (0.1, 11.795886450768533),\n",
       " (1, 16.108820229471213),\n",
       " (10, 98.037488521134946),\n",
       " (100, 115.42570237061494),\n",
       " (1000, 89.033241612769501),\n",
       " (10000, 176.89204544936493)]"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mse_for_lambda_sklearn(lam, X_train, X_test, y_train, y_test):\n",
    "    results = Lasso(alpha=lam, fit_intercept=False, max_iter=10000000).fit(X_train, y_train)\n",
    "    return(mse_from_beta_coefs(results.coef_, X_test, y_test))\n",
    "\n",
    "mses_sklearn = [mse_for_lambda_sklearn(lam, X_train, X_test, y_train, y_test) for lam in lambdas]\n",
    "list(zip(lambdas, mses_sklearn))\n",
    "\n",
    "# Side note: there's opportunity for reducing duplication with the\n",
    "# prev function by injecting the function that gets the coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mine</th>\n",
       "      <th>Sklearn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.000000e-08</th>\n",
       "      <td>15.032273</td>\n",
       "      <td>25.357574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000000e-07</th>\n",
       "      <td>14.773064</td>\n",
       "      <td>25.357483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000000e-06</th>\n",
       "      <td>13.831468</td>\n",
       "      <td>25.356579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000000e-05</th>\n",
       "      <td>14.819318</td>\n",
       "      <td>25.347543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000000e-04</th>\n",
       "      <td>12.685049</td>\n",
       "      <td>25.257431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000000e-03</th>\n",
       "      <td>12.759551</td>\n",
       "      <td>24.380344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000000e-02</th>\n",
       "      <td>16.017533</td>\n",
       "      <td>14.871339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000000e-01</th>\n",
       "      <td>15.215006</td>\n",
       "      <td>11.795886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000000e+00</th>\n",
       "      <td>14.703784</td>\n",
       "      <td>16.108820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000000e+01</th>\n",
       "      <td>12.397645</td>\n",
       "      <td>98.037489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000000e+02</th>\n",
       "      <td>14.161035</td>\n",
       "      <td>115.425702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000000e+03</th>\n",
       "      <td>55.173253</td>\n",
       "      <td>89.033242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000000e+04</th>\n",
       "      <td>71.799900</td>\n",
       "      <td>176.892045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Mine     Sklearn\n",
       "1.000000e-08  15.032273   25.357574\n",
       "1.000000e-07  14.773064   25.357483\n",
       "1.000000e-06  13.831468   25.356579\n",
       "1.000000e-05  14.819318   25.347543\n",
       "1.000000e-04  12.685049   25.257431\n",
       "1.000000e-03  12.759551   24.380344\n",
       "1.000000e-02  16.017533   14.871339\n",
       "1.000000e-01  15.215006   11.795886\n",
       "1.000000e+00  14.703784   16.108820\n",
       "1.000000e+01  12.397645   98.037489\n",
       "1.000000e+02  14.161035  115.425702\n",
       "1.000000e+03  55.173253   89.033242\n",
       "1.000000e+04  71.799900  176.892045"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mses = pd.DataFrame({'Mine': mses_myimpl,\n",
    "                     'Sklearn': mses_sklearn}, index=lambdas)\n",
    "mses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1175e4c88>"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe4AAAEzCAYAAAD3t+CnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117436160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mses.plot(logx=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*\"(e) Now generate a response vector Y according to the model Y = β0 + β7X7 + ε. Fit a LASSO model using your own coordinate descent algorithm and scikit-learn’s LASSO algorithm. Discuss the results obtained.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "b7 = 12\n",
    "new_y = b0 + (b7*(X[:,0]**7)) + e\n",
    "print(new_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.7739588404074311e-13, 9827.125459718427)"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_y_centered = preprocessing.scale(new_y, with_std=False)\n",
    "np.mean(new_y_centered), np.std(new_y_centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3.51547618e+03   2.50810819e+03  -1.60501414e+03  -1.51295923e+03\n",
      "  -4.61899979e+01   8.73073527e+01   4.89411401e+01   3.28217014e+01\n",
      "   1.10645710e+00  -1.53477527e+00]\n",
      "93851286.3787\n"
     ]
    }
   ],
   "source": [
    "# setting lambda to the highest value that gave low MSEs for both\n",
    "# my implementation and sklearn\n",
    "optimal_lambda = 1 \n",
    "\n",
    "beta_init = np.zeros(np.size(X_scaled_poly10, 1))\n",
    "betas_rand = cd.randcoorddescent(beta_init, X_scaled_poly10, new_y_centered, \n",
    "                                 optimal_lambda, max_iter)\n",
    "print(betas_rand[-1,:])\n",
    "print(mse_from_beta_coefs(betas_rand[-1, :], X_scaled_poly10, new_y_centered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2261.84105254  4506.11280851  3301.15814662 -3921.22788106 -4678.16278065\n",
      "   474.87411895  1578.32380246   200.6517575   -161.45674055   -36.63833918]\n",
      "93378652.1551\n"
     ]
    }
   ],
   "source": [
    "results_sklearn = Lasso(alpha=optimal_lambda, fit_intercept=False, \n",
    "                        max_iter=10000000).fit(X_scaled_poly10, new_y_centered)\n",
    "print(results_sklearn.coef_)\n",
    "print(mse_from_beta_coefs(results_sklearn.coef_, X_scaled_poly10, new_y_centered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Four"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Something and note here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Five"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Possibly just refer to what I submitted last week, unless something's changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Is there a concept you would love to see covered during the course?\n",
    "* Is there a concept that has been used multiple times during the course, but you still have no idea what it means?\n",
    "* Do you feel that you learn better when the course material is covered using slides, or tablet/whiteboard, or numerical illustrations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
