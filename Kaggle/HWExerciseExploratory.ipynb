{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "%matplotlib inline \n",
    "\n",
    "# fig_size = plt.rcParams[\"figure.figsize\"] \n",
    "# fig_size[0] = 12 \n",
    "# fig_size[1] = 12 \n",
    "plt.rcParams[\"figure.figsize\"] = (12,12) \n",
    "\n",
    "from sklearn import model_selection \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn import preprocessing # for scale\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import pickle\n",
    "import importlib\n",
    "\n",
    "import finalproj as fp\n",
    "import corinne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'corinne' from '/Users/andrewenfield/work/github/Data558/Kaggle/corinne.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(fp)\n",
    "importlib.reload(corinne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4320 4320\n"
     ]
    }
   ],
   "source": [
    "features_train = np.array(pickle.load(open('features_train', 'rb')))\n",
    "labels_train = np.array(pickle.load(open('labels_train', 'rb')))\n",
    "print(len(features_train), len(labels_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4320, 2048)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled = preprocessing.scale(features_train)\n",
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "_Pick two classes of your choice from the dataset. Train an L2-regularized logistic regression classiﬁer on the training set using your own fast gradient algorithm with λ = 1. Plot, with diﬀerent colors, the misclassiﬁcation error on the training set and on the validation set vs iterations._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I'll pick 086, Pacific Loon, and 087, Mallard, as the two classes.\n",
    "\n",
    "Per Zaid's answer on 5/27, in this problem I'm considering just the subset of the data that has these two classes. I pull the data and train/test a binary classifer. I only use the 60 observations for the two chosen birds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690,\n",
       "        1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701,\n",
       "        1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712,\n",
       "        1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723,\n",
       "        1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734,\n",
       "        1735, 1736, 1737, 1738, 1739]),)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_twoclasses = np.where((labels_train == '086.Pacific_Loon') | (labels_train == '087.Mallard'))\n",
    "indices_twoclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 2048)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_twoclasses = X_scaled[indices_twoclasses]\n",
    "X_twoclasses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_twoclasses = np.where(labels_train[indices_twoclasses] == '086.Pacific_Loon', 1, -1)\n",
    "y_twoclasses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_probability(logodds):\n",
    "    return 1 / (1 + np.exp(-logodds))\n",
    "    #return np.exp(logodds) / (1 + np.exp(logodds))\n",
    "\n",
    "def get_accuracy(beta_coefs, X, y_actual, prob_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Return the classification accuracy given a set of coefficients, in \n",
    "    beta_coefs, and observations, in X, compared to actual/known values \n",
    "    in y_actual. The threshold parameter defines the value above which the\n",
    "    predicted value is considered a positive example.\n",
    "    \"\"\"\n",
    "    y_pred = X.dot(beta_coefs.T).ravel() # ravel to convert to vector\n",
    "    \n",
    "    # for logistic regression convert to a prob and use a prob threshold\n",
    "    probs = get_probability(y_pred)\n",
    "    y_thresholded = np.where(probs > prob_threshold, 1, -1)\n",
    "    \n",
    "    return accuracy_score(y_actual, y_thresholded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42, 2048), (18, 2048), (42,), (18,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_twoclasses_train, X_twoclasses_test, y_twoclasses_train, y_twoclasses_test = model_selection.train_test_split(\n",
    "    X_twoclasses, y_twoclasses, test_size=0.3)\n",
    "X_twoclasses_train.shape, X_twoclasses_test.shape, y_twoclasses_train.shape, y_twoclasses_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_init = 0.01\n",
    "max_iters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2038</th>\n",
       "      <th>2039</th>\n",
       "      <th>2040</th>\n",
       "      <th>2041</th>\n",
       "      <th>2042</th>\n",
       "      <th>2043</th>\n",
       "      <th>2044</th>\n",
       "      <th>2045</th>\n",
       "      <th>2046</th>\n",
       "      <th>2047</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-0.005726</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.006333</td>\n",
       "      <td>-0.003782</td>\n",
       "      <td>-0.002165</td>\n",
       "      <td>-0.000351</td>\n",
       "      <td>-0.004308</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.002686</td>\n",
       "      <td>-0.003865</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004532</td>\n",
       "      <td>-0.000521</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>-0.007327</td>\n",
       "      <td>0.003977</td>\n",
       "      <td>0.007526</td>\n",
       "      <td>-0.010347</td>\n",
       "      <td>-0.005934</td>\n",
       "      <td>-0.001916</td>\n",
       "      <td>0.003730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-0.005726</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.006333</td>\n",
       "      <td>-0.003783</td>\n",
       "      <td>-0.002165</td>\n",
       "      <td>-0.000351</td>\n",
       "      <td>-0.004310</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.002686</td>\n",
       "      <td>-0.003867</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004532</td>\n",
       "      <td>-0.000522</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>-0.007326</td>\n",
       "      <td>0.003977</td>\n",
       "      <td>0.007527</td>\n",
       "      <td>-0.010347</td>\n",
       "      <td>-0.005934</td>\n",
       "      <td>-0.001916</td>\n",
       "      <td>0.003729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>-0.005726</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.006334</td>\n",
       "      <td>-0.003784</td>\n",
       "      <td>-0.002166</td>\n",
       "      <td>-0.000351</td>\n",
       "      <td>-0.004311</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.002686</td>\n",
       "      <td>-0.003868</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004532</td>\n",
       "      <td>-0.000523</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>-0.007326</td>\n",
       "      <td>0.003977</td>\n",
       "      <td>0.007527</td>\n",
       "      <td>-0.010347</td>\n",
       "      <td>-0.005933</td>\n",
       "      <td>-0.001916</td>\n",
       "      <td>0.003729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 2048 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6     \\\n",
       "98  -0.005726  0.000066  0.006333 -0.003782 -0.002165 -0.000351 -0.004308   \n",
       "99  -0.005726  0.000066  0.006333 -0.003783 -0.002165 -0.000351 -0.004310   \n",
       "100 -0.005726  0.000066  0.006334 -0.003784 -0.002166 -0.000351 -0.004311   \n",
       "\n",
       "         7         8         9       ...         2038      2039      2040  \\\n",
       "98   0.000136  0.002686 -0.003865    ...    -0.004532 -0.000521  0.002923   \n",
       "99   0.000136  0.002686 -0.003867    ...    -0.004532 -0.000522  0.002923   \n",
       "100  0.000136  0.002686 -0.003868    ...    -0.004532 -0.000523  0.002923   \n",
       "\n",
       "         2041      2042      2043      2044      2045      2046      2047  \n",
       "98  -0.007327  0.003977  0.007526 -0.010347 -0.005934 -0.001916  0.003730  \n",
       "99  -0.007326  0.003977  0.007527 -0.010347 -0.005934 -0.001916  0.003729  \n",
       "100 -0.007326  0.003977  0.007527 -0.010347 -0.005933 -0.001916  0.003729  \n",
       "\n",
       "[3 rows x 2048 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = fp.fastgradalgo(\n",
    "    X_twoclasses_train, y_twoclasses_train, t_init=t_init, \n",
    "    grad_func = fp.compute_gradient_logistic_regression, \n",
    "    obj_func = fp.compute_objective_logistic_regression, \n",
    "    lam=1, max_iter=max_iters)\n",
    "results[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0%\n",
      "Misclassification error: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# note use of the held out test data to get the performance metrics\n",
    "accuracy = get_accuracy(fp.get_final_coefs(results), X_twoclasses_test, y_twoclasses_test)\n",
    "print(\"Accuracy: {0:.1%}\".format(accuracy))\n",
    "print(\"Misclassification error: {0:.1%}\".format(1 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_objective_values(beta_results_df, X, y, lam):\n",
    "    return beta_results_df.apply(lambda r: fp.compute_objective_logistic_regression(r.values, \n",
    "                                                    X, y, lam), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.247721</td>\n",
       "      <td>0.328401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.135823</td>\n",
       "      <td>0.170312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Train  Validation\n",
       "0  0.693147    0.693147\n",
       "1  0.247721    0.328401\n",
       "2  0.135823    0.170312"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_vals = pd.DataFrame({'Train': get_objective_values(results, X_twoclasses_train, y_twoclasses_train, 1),\n",
    "              'Validation': get_objective_values(results, X_twoclasses_test, y_twoclasses_test, 1)})\n",
    "obj_vals[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x124712550>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtcAAALGCAYAAACOBL1jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3XmYpGdZL/5vVfV0z4RMNjJZkC0CecISAhKWKMoiXCLI\nIgJHkcMhGgUEzxE9C8L5iWJcQFDWIJuKiiwKKAIHRUXAIEJYJCx5JJIQIAQmLEkgMz3TXfX7o6pm\neoae6U7S1fW+NZ/PdXFNd1X1W0/NvJd++8793E9nMBgEAAC46brTXgAAAMwK4RoAADaIcA0AABtE\nuAYAgA0iXAMAwAYRrgEAYIMI10BrlFKeUkr591LKZ0opny6l/Fkp5dYrnr+8lHL2Kj93dinlr27C\n+55XSvmFFWt45o291kHXvVsp5T9LKR8rpdx2I6654tq/Vkp55Ojr55ZSnrhB131SKeUdN+HnH1FK\necno64eVUp67Eetacf2JfG6A9Zqb9gIA1qOU8oIkZyX5sVrrF0sp3SRPSPKvpZR711q/dKifrbVe\nlOQxN+Ht75vkU6Nr/eFNuM7BHpHkvbXW8zbwmmMPTPKZJKm1/toErn+j1FrfnuTto2/vmeSEDX6L\nRn5u4MjRcYgM0HSllFsmuSTJrWqt3zzouRcnmau1Pq2UcnmS92YYwheSvLDW+kellPsneVmt9S6l\nlPkkz0tyvyS9JB9P8t9rrdeWUk5P8sokJyXpJzk/yZ4kr02yK8lvJ9mR5MQMA+ILa61njtZxXJLL\nknxvkqOSvCzJrZNsSfLGWutvH7Tun07ywtEa/j7Je5I8ptb6Y6PnnzT+vpTyJ0muTXJmkluN/i5+\nstb67VLKvZO8JMnNRmv9n0nuOPqMO5P8cpJHJvlUrfUFpZQfTPJ7ozXuSfJ/a63vHr3fj48+9x1G\nzz2x1vqpg9b9pCS/OLr2LZJ8IcnPJZlP8ukkt6y1XlNK6SSpSR5ba/33g37+MUl+M8nfjD7/q2qt\nzy6l/GySX8jwv6p+PcnTa62XjD7/CUlul+Qdo3+Plyc5erSGTyT5L0l+dlKfG2C9tIUAbXDvJJ89\nOFiP/EOGleWxXbXW70vy4CS/W0q580Gvf2aSpST3qLWeleTKJL87eu6NSf6y1nrnJA/NMEz/Y4ZB\n+g9qrS9fcZ33JDl6RRvKTyV552iNf5bkj2qt90hyryQPKqU8buUiaq2vT/KHSd5Ua/3pdfwd3CPJ\nQzIMzrdI8thSypYkf53kubXWu2QYcl+c5BVJLkryv2qtbxtfoJRy8yR/leR/1FrvmuS/JfnzUspp\no5fcL8kvjq51YZL/dYi1nJ5h8L1rkouTvLjWesXo72r8WR6Q5Osrg/VBn//fVnz+Z5dS7jdazw/W\nWu+e5PlJ3rriR46qtd651vp/Rp/zdbXWc5LcPslpSR42+veZ5OcGWJNwDbTFlkM8vpBk5X+Ce2WS\n1FqvTPJ3SX74oNf/WIYVzY+XUj6R5FFJ7lRKOSHDivdrRj//xVrr7Wqt1672prXWQYYV1CeNHjo3\nyWtKKTfLMKz95uj6H8qwgn239X/UVb271rpYa92bYaA9IcNK9nKt9Z2jNX201npmrbV/iGvcO8ml\no2CbWuunMwyT9x89/9EV7TUfy6FbNv6h1nrp6OvXZviLTDKsJv/c6OsnZxjy1+thGQblD47+3p6f\n5ITRv0uS/MuK1/6fJDtLKf979B63yLCKfSgb9bkB1qTnGmiDDyW5QynllFrrVQc994AkH1zx/fKK\nrztJ9h70+l6GFcz/lySllKOTbM2wmp2sCOqllJLkisOs648zDOmvSXJcrfWfSynHjN73+2ut14+u\nc2KS3Wt8xsHo58bmD3p+1yqvXcqBv1iklHKXDNtGVrNaQaWb4S8uew7xHqs51N/xPyQ5qpTyw0l+\nKMMK8Xr1kvzZqDKdUU/9LZKM/2vFt1e89g0Z/v+vNyd5Z4a/vBxqrcnGfW6ANalcA41Xa/1yhn3F\nbyilfM/48VLKuUl+IsM+27EnjZ67dYYV1X886HJ/l+TppZT5UYB7dZLfGVWoP5pRICyl3CrD6uax\nGYbY76qcj9b1bxlWy8cV72sz/GXgl0fXOW50nUeu8TF3JrlLKWVrKWUuycPXeH0y7GkelFIePHqv\n70vyTxn+3/bV1vyh4cvKvUavv3OGIfif1/FeKz1gxZSWpyb5f8m+av4FGf5d/EWtda1fKFau8e+T\n/FQp5dTR90/Jd//bjf1Ihq0wb8owDN87w3B+8DXHNupzA6xJuAZaodb6q0n+PMnflFI+VUr5XJIH\nJTmn1vqFFS/dWkr5WJJ3ZdhH+x8HXeo3k1ye4UbGz2RYpfyV0XOPT/K4Usq/J/nbJOeNKuX/L8l/\nL6X86ipLe3WSuyd53YrHHp/kPqWUizMM328Y9Vgfzt8neV+GVecPZNj6cVi11sUkj07ynFErxR8m\neXStdc9o/S8opfy3Fa+/Osljk7x0tLa/SHLuKn9Ha/lkkj8qpXwqw6rxL6947k8z3HT5ynVc5x+T\nPKKU8tJa699l+EvSe0opn8zw7/DRo8B+sGcleVsp5aIMP/P7MmwpSSb7uQHWZFoIMPNKKY9I8uxa\n672nvZZZV0r5qQynbfzotNcCMA16roGZVkp5epL/m8TM4wkrpfxzkpMzbNUBOCKpXAMAwAbRcw0A\nABtEuAYAgA3Smp7rpaXlwTe/ef20l0HDHH/8UXFfcDD3BatxX7Aa9wWr2bFj+42ed9+ayvXcXG/t\nF3HEcV+wGvcFq3FfsBr3BRutNeEaAACaTrgGAIANIlwDAMAGEa4BAGCDCNcAALBBhGsAANggwjUA\nAGwQ4RoAADZIa05oBACgeV760j9IrZ/NN77x9ezevTu3uMX35Ljjjs/55z/vsD/3uc/V/Mu/vD/n\nnvtzm7TSzdEZDAbTXsN6DXbuvG7aa6BhduzYHvcFB3NfsBr3BatxX2ycd73rb/OFL1yepz71F6e9\nlJvsphx/rnINADAD3vxPl+Yjl3xtQ695zzNOyuMeePsb/HMf+9hFecUrXpotW7bkEY/48SwsLOSt\nb/3LLC0tpdPp5Ld/+wX5/Ocvzd/8zVvyG7/xO/nJn/zxnHnmWbniii/khBNOyPnnPz+9XjuPptdz\nDQDAhtuzZ08uuOA1echDHpYvfvGK/N7vvTiveMVrc9vbnpYPf/hfD3jtlVd+Oeed95S88pV/nG99\n65v57Gc/M6VV33Qq1wAAM+BxD7z9jaoyT8qtb32bfV8ff/wJOf/85+Soo47KF75wee5yl7se8Npj\njz0uJ598SpLkpJNOzp49i5u61o0kXAMAsOG63WHb8re//e289rWvzFve8o4kyTOe8bQcvOev07nR\nLc6NI1wDADAxN7vZzXLmmWflKU85N73eXLZv356rr96ZU0+9xbSXNhGmhdBqdnmzGvcFq3FfsBr3\nBau5KdNCbGgEAIANIlwDAMAGEa4BAGCDTGxDYymlm+SCJGclWUxyXq310tFzpyR544qX3y3JM2ut\nfzip9QAAwKRNclrIo5JsrbWeU0q5T5IXJnlkktRar0py/yQppZyT5LeSvHqCawEAgImbZFvIfZO8\nO0lqrR9KcvbBLyildJK8NMlTa63LE1wLAABM3CQr18ckuWbF98ullLla69KKxx6e5NO11rqeC+7Y\nsX0j18eMcF+wGvcFq3FfsBr3xU3zhCc8IU972tNyzjnn7Hvs/PPPTyklj33sYw947Ze+9KX88i//\nct785jfnGc94Rp73vOdlfn5+3/Pvf//78653vSu/+7u/u+p7LS4u5u1vf3se+9jH5q1vfWuOPfbY\n/PAP//BkPtiNNMlwfW2SlXdr96BgnSRPSPLi9V7QHEoOZj4pq3FfsBr3BatxX9x0D3nIw/OmN/1V\nbn/7uyRJ9u7dm3/4h3/KE5/489/1d/uNb3wne/cuZ+fO6/KsZz0311yzmOHWvKFrrtmV3bv3HvLf\n5CtfuTJ/8RdvzP3v/5D84A8+OMlk8uFN+YVrkuH6wgwr028e9VxfvMprzk7ywQmuAQDgiPDWS9+R\nj39ttbh14939pDPz6Nv/2GFfc//7/3Be+cqXZ/fu3dm6dWs+8IH35V73uncuueQz+eM/fnX6/X52\n7dqV5zzn/GzZsmXfzz3mMQ/P61//V/nKV67M7/zOc7N167Zs27Y127cfkyR5y1velPe9773ZtWtX\njjvuuPz2b78gf/qnf5TLL79s33VvfvOb51GPekxe+tI/yCc/+YkkyYMf/JA87nE/ld/6rV/Pli1b\nctVVX8nXv351nvWsX08pZ2zo389qJtlz/bYku0spH0zyB0meUUp5fCnl55OklLIjybW11tYcEQkA\nwIEWFhbyQz90/7z//e9NkrzrXW/PIx/56Fx22efza7/2m3nZy16V+93vAXnve/9h1Z+/4IIX57zz\nnpwXv/iC3OUud02S9Pv9XHPNNXnRiy7Iq1/9uiwvL+ezn/10nvjEn8ltb3tazj335/b9/IUXfiBf\n+cqVedWr/iSveMVr8573vDv/+Z+XJklOOeXU/P7vvyw/8RP/JW9/+1sn/DcxNLHKda21n+QpBz18\nyYrnd2Y4gg8AgJvo0bf/sTWrzJPy8If/eF7+8hfn7ne/R6677rqcfvoZ+epXr8qLXvR72bbtqOzc\n+bWceeZZq/7sFVdckTvecdhScuaZd8sXvnB5ut1utmzZkl//9Wdn27Zt+drXvpalpYO7i4e+8IXL\nctZZd0un08nc3FzufOczc/nln0+S3OEOJUly0kkn5+KL/30Cn/y7teYQmf/+ppdkqW+gCABA09zu\ndrfPrl3fyV/+5RvzsIc9IknyvOf9Vp71rOfk2c/+9Zx44o5D/uxpp52WT33qk0mSSy75dJLk0ks/\nl/e//5/z3Of+Tp7xjP+dwaCfJOl0uvu+HrvNbU7b1xKytLSUT33qk7nlLW89en1nYz/oOkyy53pD\nXZXP5rKdX80dTr7FtJcCAMBBHvawR+TlL39J3vKWdyRJfuRHfjS/8As/l23btub442+eq6/euerP\nPf3pz8j55z8nb3jDn+W4447L/PxCbnnLW2Xbtm156lN/Jkly85ufmKuv3pk73/nM7N27lAsueEkW\nFhaSJD/wAz+Yj3/8o3nyk8/N3r1788AHPmhTeqsPpTMYtKPl+XFveurgaXd8eu506q2nvRQaxC5v\nVuO+YDXuC1bjvmA1O3Zsv9El79a0hSTRFgIAQKO1KlzvXRauAQBorlaFa5VrAACarF3hWuUaAIAG\na1e4VrkGAKDBWhWu9/ZXHx4OAABN0KpwvaxyDQBAg7UqXGsLAQCgyVoVrpf7/bVfBAAAU9KqcL1X\n5RoAgAZrVbheHgjXAAA0V7vCtco1AAAN1qpwbUMjAABN1qpwrXINAECTtStc67kGAKDBWhaujeID\nAKC5WhauVa4BAGiudoVrh8gAANBg7QrXKtcAADRYq8J1X7gGAKDBWhWuVa4BAGiyVoXrvmkhAAA0\nWKvCtVF8AAA0WavCtco1AABN1rJwrecaAIDmale4jso1AADN1a5wrS0EAIAGa1m41hYCAEBztStc\nawsBAKDBWhWuB8I1AAAN1qpwrecaAIAma1e4VrkGAKDBWhOuB/2OthAAABqtNeE6g462EAAAGq1F\n4bqrcg0AQKO1KFxrCwEAoNlaFK5VrgEAaLbWhOvBoJNBR7gGAKC5WhOutYUAANB0LQvXg2mvAgAA\nDqk14boTPdcAADRba8J1Bt1EzzUAAA3WmnDdiZ5rAACarTXheli51nMNAEBztSZcdzIM14OBgA0A\nQDO1K1wn6Q+0hgAA0EytC9fLg+UprwQAAFbXonDdSSJcAwDQXC0K16PKdV9bCAAAzdSicN1LonIN\nAEBztSZcd21oBACg4VoTrjsdPdcAADRba8J1d1/PtXANAEAztShcj3uutYUAANBMrQnXnc5wqUv9\npSmvBAAAVteacD1uC1kyig8AgIZqTbjudYZtIXuXVa4BAGim1oTr7ihcL9nQCABAQ7UoXA+XundZ\nuAYAoJlaF65taAQAoKlaE657+8K1yjUAAM3UmnC9r+daWwgAAA3VmnC9b1qIthAAABqqPeG6a1oI\nAADN1ppw3dVzDQBAw7UmXI83NC4L1wAANFRrwvVcdy6JDY0AADRXa8L1vp7rgXANAEAztSdc72sL\n6U95JQAAsLr2hOtR5VrPNQAATdWacL2v53pgzjUAAM3UmnA9PkRGWwgAAE3VmnA91x31XNvQCABA\nQ7UoXOu5BgCg2doTrnvjUXzaQgAAaKbWhOvxtJC+thAAABqqNeF6y2haiLYQAACaqjXheq43Ctfa\nQgAAaKjWhOst4w2NwjUAAA3VmnA93tCo5xoAgKZqUbjWFgIAQLO1JlxvUbkGAKDh2hOuR9NC+irX\nAAA0VHvC9bhyHZVrAACaqT3hem4ug4HKNQAAzdWacN3tdpJBR7gGAKCx5iZ14VJKN8kFSc5Kspjk\nvFrrpSuev2eS30/SSXJVkifUWncf6nq9bicZdNOPcA0AQDNNsnL9qCRba63nJHlmkheOnyildJK8\nOsm5tdb7Jnl3ktsc7mI9lWsAABpuYpXrJOPQnFrrh0opZ6947vQkX0/yjFLKXZK8s9ZaD3exXreb\nDDpJt58dO7ZPbNG0j/uB1bgvWI37gtW4L9hIkwzXxyS5ZsX3y6WUuVrrUpITk3x/kqcnuTTJO0op\nF9Va/+lQF+v1hm0hy/3l7Nx53QSXTZvs2LHd/cB3cV+wGvcFq3FfsJqb8gvXJNtCrk2ycmXdUbBO\nhlXrS2utn6217s2wwn32wRdYqdvtZKAtBACABptkuL4wyUOTpJRynyQXr3ju80mOLqXcfvT9Dyb5\n9OEuNt7QOLChEQCAhppkW8jbkjy4lPLBDCeCnFtKeXySo2utryql/GySvxhtbvxgrfWdh7vYeEPj\nwCEyAAA01MTCda21n+QpBz18yYrn/ynJvdZ7vV5vuKHRKD4AAJqqNYfIaAsBAKDpWhOuu/vaQoRr\nAACaqTXhujeaFjLIYNpLAQCAVbUoXHeTQTfpDIzjAwCgkdoTrnvDtpAkWRauAQBooPaE69GGxiQq\n1wAANFJrwvV4Q2OSLPfNugYAoHlaE66HPdfjthDhGgCA5mlRuB5OC0mEawAAmqk14brb7aQz6rle\n7uu5BgCgeVoTrpPs29Cocg0AQBO1Klx3MmwL6QvXAAA0UMvC9bhyrS0EAIDmaWm4VrkGAKB52hmu\nbWgEAKCB2hmuVa4BAGigVoXrbmd8/LlwDQBA87QqXJtzDQBAk7UqXI8r19pCAABoolaFa6P4AABo\nslaFa5VrAACarF3herTcfl+4BgCgeVoWrntJtIUAANBM7QrX2kIAAGgw4RoAADZIu8L1qC1kSc81\nAAAN1K5wPapcC9cAADRRO8P1snANAEDztCpc9zraQgAAaK5WhWttIQAANFmrwrXKNQAATdaycD0a\nxSdcAwDQQO0K112VawAAmqtd4bozPv5cuAYAoHnaFa672kIAAGiudoXrzlwSbSEAADRTy8L1qHI9\n6E95JQAA8N3aFa5taAQAoMFaFa7nujY0AgDQXK0K16aFAADQZK0K1+PKdb+v5xoAgOZpZbhWuQYA\noIlaFa57+8K1yjUAAM3TqnA91+tmMEj6KtcAADRQq8J1t9NJBl2VawAAGqlV4brX7SSDjso1AACN\n1Kpw3e2qXAMA0FytCtf7K9fCNQAAzdOqcN0dh+toCwEAoHlaFa573U4GKtcAADRUq8L1uOdauAYA\noIlaFa572kIAAGiwVoXr8Zzrgco1AAAN1Kpwvb9yLVwDANA8rQrX42khA+EaAIAGalW4Hk4L6apc\nAwDQSK0K1+PKdRITQwAAaJxWheveinC93DcxBACAZmlVuB7PuU6S5YFwDQBAs7QvXPdHlWttIQAA\nNEyrwvV4Q2Oicg0AQPO0KlwPD5GxoREAgGZqVbjudTtJbGgEAKCZWhWuhz3X2kIAAGim9oXrgQ2N\nAAA0U6vCda9jzjUAAM3VqnDdNS0EAIAGa1W47mkLAQCgwVoVrlee0GgUHwAATdOqcH1A5VrPNQAA\nDdOqcH3gtBDhGgCAZmlVuO6taAsRrgEAaJpWhevhtBAbGgEAaKZ2hWtzrgEAaLBWhetOp5NOtIUA\nANBMrQrXSfaFa6P4AABomtaF667KNQAADdW+cN0Zheu+yjUAAM3SvnCtcg0AQEO1Llx3Or0kwjUA\nAM3TunA9rlz3tYUAANAwrQ3XKtcAADRN+8K1thAAABqqheF6XLnWFgIAQLO0Llz3OtpCAABoptaF\n63Hl2oZGAACapoXhWs81AADN1LpwrS0EAICmmpvUhUsp3SQXJDkryWKS82qtl654/hlJzkuyc/TQ\nk2utda3r7qtcawsBAKBhJhaukzwqydZa6zmllPskeWGSR654/h5Jnlhr/egNuWjPnGsAABpqkm0h\n903y7iSptX4oydkHPX+PJL9aSvmXUsqvrvei3e64ci1cAwDQLJOsXB+T5JoV3y+XUuZqrUuj79+Y\n5OVJrk3ytlLKj9Va33G4C+7YsT3bFuaTJL0tnezYsX0Cy6Zt3Aesxn3BatwXrMZ9wUaaZLi+NsnK\nu7U7DtallE6SF9Varxl9/84kd09y2HC9c+d1WV4aJEm+s3sxO3deN4l10yI7dmx3H/Bd3Besxn3B\natwXrOam/MI1ybaQC5M8NElGPdcXr3jumCSfKqUcPQraD0yyrt7ruY62EAAAmmmSleu3JXlwKeWD\nSTpJzi2lPD7J0bXWV5VSnpXkvRlOEvnHWuu71nPRnp5rAAAaamLhutbaT/KUgx6+ZMXzf5bkz27o\ndXvdYbF9aWAUHwAAzdK+Q2S63Qz6HZVrAAAap33hutNJ0klf5RoAgIZpXbjudjvJoOMQGQAAGqd1\n4brX7SSDrnANAEDjtC5cd7udpN/Jcl9bCAAAzdLKcD1QuQYAoIFaF657o57rvnANAEDDtC5c79/Q\nqC0EAIBmaV24Hm9oNIoPAICmaV247nZGbSERrgEAaJbWhWs91wAANFXrwvV4Woi2EAAAmqaV4Xrc\nFjIYDKa9HAAA2Kd14brXGW5oTKJ6DQBAo7QuXI8r10kcJAMAQKO0Llz3DgjXKtcAADRH68J1t7u/\nLUTlGgCAJmlluB6MK9d9lWsAAJqjdeF6ZVuIWdcAADRJ68K1thAAAJqqdeG61+3u39DYF64BAGiO\n1oXrbmdl5VrPNQAAzdG6cG0UHwAATdW6cL1yWogNjQAANEnrwnXPCY0AADRU68L1AdNCbGgEAKBB\nWheu9VwDANBUrQvX5lwDANBUrQvXB1SutYUAANAgrQvX3c7KaSHaQgAAaI7WheuethAAABqqdeG6\na0MjAAAN1bpwrecaAICmal24Ni0EAICmal24NucaAICmal247nY7GahcAwDQQK0M1zGKDwCABmpd\nuLahEQCApmpduO52bGgEAKCZWheubWgEAKCpWheuu9pCAABoqFaGa9NCAABoovaF604nHW0hAAA0\nUOvCdZJ0O8NlG8UHAECTtDpc67kGAKBJWhmue51eEj3XAAA0SyvDdSc2NAIA0DytDNe9jCvXeq4B\nAGiOVobrfRsa9VwDANAgrQzXva62EAAAmqeV4brb0RYCAEDzCNcAALBBWhmu97WF6LkGAKBB2hmu\nO92k30lfzzUAAA3SznDd7WQw6NrQCABAo7QyXHe7SQYdPdcAADRKS8N1Zxiu9VwDANAgrQzXvU4n\nGXRVrgEAaJRWhutut5PBoKPnGgCARmlluO5pCwEAoIFaGa673a62EAAAGqeV4Xpf5VpbCAAADdLK\ncD2cFtLVFgIAQKO0OFx30tcWAgBAg7QyXPdG00KEawAAmqSV4brbGVWu089gMJj2cgAAIElLw3Vv\n1HOdxKZGAAAao5XhetxzncQ4PgAAGqOV4fqAyrWJIQAANMTcWi8opdwmyWuS3DbJDyV5fZKfqbVe\nPtGVHcaBlWvhGgCAZlhP5fqVSX4vyXVJrkryhiR/OslFrWU8LSQRrgEAaI71hOsTa61/n6RTax3U\nWl+d5JgJr+uwuivaQozjAwCgKdYTrneVUm6ZZJAkpZT7Jlmc6KrW0FvZFtIXrgEAaIY1e66T/HKS\ndyS5XSnlE0lOSPK4ia5qDeM514m2EAAAmmPNcF1r/Ugp5Z5JTk/SS3JJrXXPxFd2GOZcAwDQROuZ\nFvLHGbWErHgstdafmdiq1mBaCAAATbSetpB/XvH1liSPSHLJRFazTsNpIeM513quAQBohvW0hbxu\n5fellNcmuXBiK1oHlWsAAJroxpzQeMckp270Qm6IleHaKD4AAJpiPT3X/Qx7rjujh3Ym+dVJLmot\nNjQCANBE62kLuTHV7Yk6oC2kL1wDANAMhwzXpZRfO9wP1lqfu/HLWZ/eAXOutYUAANAMh6tcdw7z\n3FR1V04L0RYCAEBDHDJc11p/Y7XHSymdJKdNbEXr0Ot2kv5oQ6O2EAAAGmI9GxqfnuS3k9xsxcOX\nJbn9pBa1lu4BGxq1hQAA0Azr2az4K0nOSvKmJLdL8rNJ/m2Si1pLr9vJuGtFuAYAoCnWE66/Vmu9\nLMknk5xZa/2TJGWiq1qDQ2QAAGii9YTr75RSHpBhuH54KeWUJMdPdlmH1+t2Mujb0AgAQLOsJ1z/\nYpJHJHl3kpsnuSTJSye5qLUccEKjDY0AADTEmhsak9whyf+utfaT/MR6L1xK6Sa5IMN+7cUk59Va\nL13lda9K8o1a6zPXe21zrgEAaKL1VK5/OsllpZQ/LKXc9wZc+1FJttZaz0nyzCQvPPgFpZQnJznz\nBlwzycHTQlSuAQBohjXDda31sUnumOTCJM8spVxSSvnNdVz7vhm2kqTW+qEkZ698spTy/UnuneSV\nN3TRvW7X8ecAADTOetpCUmu9rpRyYZJbjf53zjp+7Jgk16z4frmUMldrXSqlnJrkOUl+PMnj1rvY\nHTu2J0m+uWtpX7jeetTcvsc5Mvn3ZzXuC1bjvmA17gs20noOkfmVJD+ZZCHJnyd5WK31S+u49rVJ\nVt6t3VoijtjHAAAgAElEQVTr0ujrxyY5Mcm7kpyS5KhSyiWjMX+HtHPndcMLX7Nr3/Hn1317977H\nOfLs2LHdvz/fxX3BatwXrMZ9wWpuyi9c66lc3yLJz9VaP3EDr31hkocneXMp5T5JLh4/UWt9SZKX\nJEkp5UlJzlgrWK9kzjUAAE20Zriutf7Kjbz225I8uJTywQyPUzy3lPL4JEfXWl91I6+ZxIZGAACa\naV091zfGaHTfUw56+JJVXvcnN/TavZWVaxsaAQBoiPWM4mucA9tCzLkGAKAZ1lW5HrVz3DnJbyV5\nTK31Tye6qjUMD5HRFgIAQLOsWbkupfxukocmeXSGYfzcUsp3HQizmbrdTgbj489VrgEAaIj1tIX8\nSJL/mmR3rfXaJA9O8qMTXdUaunquAQBooPWE63FpeDD6c2HFY1PRO2BaiMo1AADNsJ5w/eYkb0py\nQinll5K8P8lfTHRVa+h2zLkGAKB51jPn+nmllB9J8oUkt07ynFrrOya+ssM4oHKtLQQAgIZYz/Hn\nf53hsefPrrXumfyS1uaERgAAmmg9bSGvTvKoJP9ZSnlNKeX+k13S2nrdTpJhwNZzDQBAU6wZrmut\n76y1PiHJ6UneneSFpZQvTHxlh9Hp7PvKKD4AABpjvYfI3CnJTyZ5bJIvJnnRJBe1lk6nk163k86g\nqy0EAIDGWE/P9cVJljLsu35grfUrE1/VOnRHrSE2NAIA0BTrqVw/vtZ68cRXcgN1RxND9FwDANAU\nhwzXpZRX1Vp/PslLSimDg5+vtT5woitbQ68z3tCocg0AQDMcrnL9ytGfv74J67jBut1OBoOuthAA\nABrjkOG61vrR0ZePqbX+4srnSimvS/K+SS5sLb1uJ0uDTvoq1wAANMTh2kJek+R7k5xdSrnziqe2\nJDl20gtby7Bybc41AADNcbi2kPOT3DbJizNsDRlPl15K8tmJrmodevs2NO6d9lIAACDJYQ6RqbVe\nXmv95yT3TXJmrfV9SS5N8iNJdm/O8g5tfAS6DY0AADTFeo4/f32SU0dfXzf6mT+b2IrWqTcK130b\nGgEAaIj1zLm+Ta31EUlSa702yf8tpXxissta275pIXquAQBoiPVUrgellDPH35RSzkgy9UbnXqeT\n9DsZZJC+gA0AQAOsp3L9P5O8p5TypQw3NZ6Y5AkTXdU6jKeFdJIs95fT7a3n9wQAAJicNcN1rfUf\nSim3TnJmhhXrWmtdnPjK1tBbGa4H/WyZ9oIAADjirVnuLaUcn+TlSX4vyZeTvGL02FR1u50M+sPp\ngA6SAQCgCdbTS/HqJB9JcvMMp4V8JcmfT3JR6zGeFpLEpkYAABphPeH6tFrrq5L0a617aq3PTnLL\nCa9rTeNpIUnMugYAoBHWE66XSinHJhkkSSnlDkmmXirurqxcm3UNAEADrGdayHOS/HOSW5dS/jrJ\nOUl+ZpKLWo9ep5Msq1wDANAc65kW8u5SykVJ7p2kl+TJtdavTnxla+h2O8mSnmsAAJrjkOG6lPLz\ntdZXlVJ+7aCn7lZKSZLvJPnbWut/THKBh9LrdjIcux2HyAAA0AiH67nurPhztf99T5K/m+jqDkPP\nNQAATXPIynWt9ZWjP3+jlLIlyfjY88/VWpeTpJQy2JRVrqLX7WTQ13MNAEBzrOcQmR9K8p9J/jjJ\nXyS5pJRydpLUWn9lsss7tK451wAANMx6poX8QZKH1VovTpJRsL4gyb0mubC1DA+RGVWutYUAANAA\n65lznXGwHn19UdYXyieq2+2uqFwL1wAATN/hpoX80OjLS0opf5jktUmWkvx0kg9vwtoOq9dZUbkW\nrgEAaIDDVaB/46Dvn7/i66ltZBwbHn9uFB8AAM1xuGkhD9jMhdxQPRsaAQBomMP2To9aQ/6/JPcc\nPfSRJM+ttX5g0gtbiznXAAA0zSE3NJZSHpjkDUnemuQHkjwgyV8neWMp5f6bsrrD6Hb1XAMA0CyH\nq1w/J8MRfJ9Y8djHSykfynA83w+t/mOb48C2EOEaAIDpO9wovmMOCtZJklrrR5OcMLklrc8Bleu+\nnmsAAKbvcOH66FLKd1W2R49Nfc51b8W0EJVrAACa4HDh+u+SPG/lA6WUXoYtIe+c5KLWo9vZ3xZi\nFB8AAE1wuAr0/0nyt6WUS5OMT2U8O8mnkzx6E9Z2WDY0AgDQNIebc/2dJA8spdwvw1F8gyQvqrX+\ny2Yt7nAO2NCo5xoAgAZYs3e61vq+JO/bhLXcIF3TQgAAaJjD9Vw3Wk9bCAAADdPacN3tmBYCAECz\ntDZcr6xc9/VcAwDQAK0N1wf2XAvXAABMX2vDtePPAQBomtaGa3OuAQBompaHa3OuAQBojtaG6153\n/7SQvso1AAAN0NpwrS0EAICmaW247nVsaAQAoFlaG66N4gMAoGlaG64dIgMAQNO0Nlx3zbkGAKBh\nWhuue91OkmHAFq4BAGiC1obrbndYte6ka841AACN0Npw3VsZrlWuAQBogNaG63Hlupte9vaXprwa\nAABocbjudUbhejCXPct7prwaAABocbjeX7mey+Ly4pRXAwAALQ7X+3quVa4BAGiI1obr7opwvTRY\nznLfpkYAAKarteF6XLnu9ntJkj191WsAAKarteF6XLnOYC5Jsqg1BACAKWttuN7Xc90XrgEAaIbW\nhuv9letRW4hwDQDAlLU3XHdUrgEAaJbWhutOpzMM2H2VawAAmqG14ToZtYYI1wAANESrw3Wv28lg\neRiutYUAADBtrQ7XKyvXwjUAANPW6nC9snLtEBkAAKat1eG6qy0EAIAGaXW4PrDnenHKqwEA4EjX\n6nDd7XTSXxpPC9k75dUAAHCka3W47nU76RvFBwBAQ7Q6XHe7nfSXhh9BzzUAANPW6nDdWxGuVa4B\nAJi2VofrbreT/nI3nXRsaAQAYOraH677yZbeFpVrAACmrtXhutftZLk/yEJvPosOkQEAYMpaHa6H\nletBFrrzRvEBADB1c5O6cCmlm+SCJGclWUxyXq310hXP/0SSZyYZJHl9rfXFN/Q9up1O+oNB5nvz\n+c7iNRu0cgAAuHEmWbl+VJKttdZzMgzRLxw/UUrpJfndJA9Kck6SXyilnHhD36DX7SRJ5nvzNjQC\nADB1E6tcJ7lvkncnSa31Q6WUs8dP1FqXSyl3rLUulVJOStJLsmbT9I4d2w/4fuvCcPk3W9iW/rX9\nHH/Ctsz1JvmRaKKD7wtI3Beszn3BatwXbKRJJtFjkqzs1VgupczVWpeSZBSsH53k5UnemeQ7a11w\n587rDvh+ebk/+mJYgP/yV7+eo7YctQFLpy127Nj+XfcFuC9YjfuC1bgvWM1N+YVrkm0h1yZZubLu\nOFiP1VrfmuR7kswneeINfYNuZ9QW0p1P4pRGAACma5Lh+sIkD02SUsp9klw8fqKUckwp5X2llIVa\naz/DqnX/hr7BuOd6S3dLEqc0AgAwXZNsC3lbkgeXUj6YpJPk3FLK45McXWt9VSnl9UneX0rZm+ST\nSf78hr5BdxyuO8NwrXINAMA0TSxcjyrSTzno4UtWPP+qJK+6Ke8xrlzPaQsBAKABWn+ITLK/cr3H\nKY0AAEzRTITrua62EAAApq/V4XpfW0iEawAApq/V4Xpf5bpjWggAANPX6nDd6wjXAAA0R6vD9bhy\n3esMh55oCwEAYJpaHa7HPde9qFwDADB9rQ7X+yrXGVeuF6e5HAAAjnCtDtcHV64Xl/dOczkAABzh\nWh2uuwe3hThEBgCAKWp1uB5XrruDYVuInmsAAKap1eF6XLnOoJtOOqaFAAAwVa0O1+M514NBstCb\nt6ERAICpanW4Hleul/uDLPTmtYUAADBVrQ7X457r/mCQeeEaAIApa3W4Xlm5nu/NG8UHAMBUzUS4\n7o/aQhaXFzMYDKa8KgAAjlStDte9lZXr7nwGGWSpvzTlVQEAcKRqdbg+oHI9t5AkWXSQDAAAU9Lq\ncN3rDpc/rlwnDpIBAGB6Wh2uu52VPdejI9CFawAApqTV4bp30LSQJE5pBABgalodrrsr5lwvCNcA\nAExZq8N174ATGocbGrWFAAAwLa0O1yunhWgLAQBg2lodrlfruVa5BgBgWlodrg8+oTFRuQYAYHpa\nHa57q4RrlWsAAKal1eF6POd65SEyTmgEAGBaWh2uVa4BAGiSVofrcc/18sC0EAAApq/V4Xp/5bq/\nYkPj4jSXBADAEazV4bp7wCEy47aQvdNcEgAAR7BWh+veKofI6LkGAGBaWh2uV1au57pz6Xa6eq4B\nAJiadofrzv7KdZIs9Ob1XAMAMDXtDtcrKtdJMt+d1xYCAMDUtDpcr+y5TpKFuXmHyAAAMDWtDtcr\n51wnyYLKNQAAU9TqcH1w5Xq+N589y3szGIVtAADYTK0O191VwvUgg+ztm3UNAMDma3e47nTSyf4N\njQu9hSSOQAcAYDpaHa6TYfV65Si+xEEyAABMR+vDda/b2T+KbxSuVa4BAJiG1ofr7gHhekuSZI9x\nfAAATEHrw/X8XDd7lvpJhqP4kmRxSbgGAGDztT5cb9u6Jbt2D6eDLMwNNzSqXAMAMA2tD9dHLczl\n+sXlJMPjzxM91wAATEf7w/XWuSwt97N3adm0EAAApqr94XphLkly/e4l00IAAJiq9ofrraNwvShc\nAwAwXe0P1ysq19pCAACYptaH620L+yvXwjUAANPU+nA9bgvZpS0EAIApa3+4XqUtZHF5cZpLAgDg\nCNX+cL11lbYQh8gAADAF7Q/XC1uSjEbx7TtEZu80lwQAwBGq9eF624rKda/by1ynZ0MjAABT0fpw\nvb/nelitnu/NC9cAAEzFzITrXYvLSYbh2oZGAACmofXhen5LN71uJ9cvDivXC70Fo/gAAJiK1ofr\nTqeTbQtzuX73UpJkobdFWwgAAFPR+nCdDMfxXb84DNfzvfns6e9Nf9Cf8qoAADjSzEa4XpjLrt37\nw3WS7O0vTXNJAAAcgWYiXG9bmMuepX6WlvtZ6DqlEQCA6ZiJcH3gKY0LSaLvGgCATTcb4Xo8jm/3\n0r62EBNDAADYbLMRrg+oXA/Dtco1AACbbTbC9b5TGpcy39uSROUaAIDNNxvheuswUK/suRauAQDY\nbDMRrrct9JIk1+/eu6/nWlsIAACbbSbC9VELw8r1rsVlPdcAAEzNbITrfRsa91euF/vCNQAAm2s2\nwvWKDY37DpFZEq4BANhcsxGuV47imxu1hahcAwCwyWYiXG9bOYqv6xAZAACmYybC9db5XjqdYeXa\ntBAAAKZlJsJ1p9PJUQtz2eWERgAApmgmwnUybA0ZntA4bgtZnPKKAAA40sxMuD5q69zohEY91wAA\nTMfshOuFuSzuWc5gkGzpzmXP8t5pLwkAgCPM7ITrrftPaZzvzTtEBgCATTc74XrfOL69me/OZ3FJ\nzzUAAJtrZsL1eNb1rsXlLMwtOEQGAIBNNzPhet8pjbv3ZqE7bxQfAACbbnbC9cL+I9Dne1uyt7+U\n/qA/5VUBAHAkmZ1wvXX/EegOkgEAYBpmJ1wfULk26xoAgM03N6kLl1K6SS5IclaSxSTn1VovXfH8\nTyX5pSRLSS5O8gu11hvdx3FA5Xr7QhLhGgCAzTXJyvWjkmyttZ6T5JlJXjh+opSyLcn5SR5Qa/2B\nJMcm+bGb8mb7p4Xsr1xrCwEAYDNNMlzfN8m7k6TW+qEkZ694bjHJ99darx99P5dk9015s5VtIft6\nro3jAwBgE02sLSTJMUmuWfH9cillrta6NGr/+GqSlFJ+McnRSd6z1gV37Nh+yOe2Hb11+CaD5Pjt\nR48emzvszzAb/BuzGvcFq3FfsBr3BRtpkuH62iQr79ZurXVp/M2oJ/v5SU5P8hO11sFaF9y587pD\nPtcfDNJJ8s1rd2dp9/BSX/3Gt7Kzd+ifof127Nh+2PuCI5P7gtW4L1iN+4LV3JRfuCbZFnJhkocm\nSSnlPhluWlzplUm2JnnUivaQG63b6WTrwtxoFN9wQ6OeawAANtMkK9dvS/LgUsoHk3SSnFtKeXyG\nLSAXJfnZJB9I8k+llCR5ca31bTflDY9amMuuxb2Z7x2VxLQQAAA218TC9aiv+ikHPXzJiq83vGp+\n1Na5XH3NLtNCAACYipk5RCYZjuPbvbicLd0tSYRrAAA210yF66MW5jJIkn4vibYQAAA212yF69Ep\njYMl4RoAgM03W+F6dJDM8tLwY2kLAQBgM81WuB5Vrpf2DD/WohMaAQDYRLMVrkeV6717O0mSxeXF\naS4HAIAjzEyF623jcL1nGK61hQAAsJlmKlyP20J2LS5nvrtFuAYAYFPNVrgeVa6vX1zKfG8+i8t7\np7wiAACOJLMVrrcOD4+5fvdSFnrzKtcAAGyqmQrX27aOK9d7R5VrGxoBANg8MxWu97WF7F7KQm9B\n5RoAgE01U+F628LwZMZdo57rpcFylvvLU14VAABHipkK171uNwvzvVy/uJSF3rD/eo+DZAAA2CQz\nFa6TYWvI9buXMt+dT5Isag0BAGCTzF643jqXXYvDaSGJcA0AwOaZvXC9MLdvznXilEYAADbPTIbr\nwSDpZdhzrXINAMBmmblwPZ513RkMJ4eoXAMAsFlmLlyPZ11nefinyjUAAJtl9sL1qHI96KtcAwCw\nuWYvXC8Me60Hy8NwrXINAMBmmb1wPapcLy91kjhEBgCAzTN74XrUc728d/jRdi3tnuZyAAA4gsxc\nuB5PC9mydGy6nW4+8/VLprwiAACOFDMXrvdVrvfM504nlFxx3Zdz5bevmvKqAAA4EsxsuL5+91Lu\nfeo9kiT/dtVHp7kkAACOEDMXrsdtIdcvLuXME++Uo+a25cNXfSzL/eUprwwAgFk3c+F6ZeV6S3cu\nZ598t1y757pc8s3PTXllAADMupkL13O9bua3dHP94lKS7GsN+dBXLprmsgAAOALMXLhOhtXrXaNw\nfZvtt8opR52UT179mVy/9/oprwwAgFk2k+F628Jcrt89DNedTif3PvUeWeov5aNf+/cprwwAgFk2\nk+H6qK3DyvVgMEiS3OuU70snnfzbV0wNAQBgcmYzXC9syXJ/kD17+0mS4xaOzRkn3CGXXXtFvvqd\nr015dQAAzKrZDNcrxvGN3eeU0cZGM68BAJiQ2QzX+8bx7d332F133CVbe1vz4as+lv6gP62lAQAw\nw2YzXI8q17sW9x8cM9/bknucfNd8a/Ga1G9eOq2lAQAww2YyXG8bV64X9x7w+H1OPTtJbGwEAGAi\nZjJcrzylcaXTjrlNTtp2Yj6x81PZtbR7GksDAGCGzWa4XmVDY7J/5vXe/t58/GufnMbSAACYYbMZ\nrg9RuU72z7z+kNYQAAA22EyG622HqFwnyQlbj88djr9d/vOay3Llt6/a7KUBADDDZjJcjyvXu1YJ\n10nygFv+QJLkrZe+Y98pjgAAcFPNdLherS0kSc488U654wmn57Pf+I9cfPVnNnNpAADMsNkM14dp\nC0mGGxsfc4eHp9vp5i2f+9vsXd676usAAOCGmMlwvWWul7le95CV6yQ55WYn5/63/IFcvfsb+ccv\nfmATVwcAwKyayXCdDKvXh6pcjz30tAdl+5aj83eX/2O+uftbm7QyAABm1eyG64W57Np9+HaPbXPb\n8ojb/Wj29Pfmr//zXZu0MgAAZtXshuutc7l+cXnN193n1Hvk1ttvmYu++olc+q3LNmFlAADMqtkN\n1wtzWVruZ+/S4QN2t9PN405/ZJLkL//jb9If9DdjeQAAzKCZDdfb1hjHt9Jpx94m9z7lHvnSt6/M\nhVd+eNJLAwBgRs1suF5rHN/BHnm7H83W3kL+9vPvznf2Xj/JpQEAMKNmN1zfgMp1khy7cEx+9LQH\n5Tt7r887Pv93k1waAAAzanbD9Q2sXCfJ/W/5Azn5qB15/5f/NR/72icntTQAAGbU7IbrUeV61w0I\n13PduZx3l/+ahd58/vQzb8oV131pUssDAGAGzWy43rb1hrWFjN3i6FPypDv9VJb6S3nlJ1+Xaxav\nm8TyAACYQTMbrvf1XN+AyvXYXXfcOY/43ofkW4vX5NUXvy57lw9/GA0AACQzHK6P3741SfKlnd++\nUT//4NvcP/c8+e657Nor8ob61gwGg41cHgAAM2hmw/Utd9wsJx67Nf9+6dVrHiSzmk6nk8ef8Zjc\nZvut8m9XfTT/+MX3T2CVAADMkpkN151OJ2eXk7JrcTmfuuwbN+oa870t+fm7PjHHzh+Tv770XfnU\n1Z/d4FUCADBLZjZcJ8nZZ5yUJLnokq/d6Gsct3BsnnzX/5a5bi9//Ok35MpvX7VRywMAYMbMdLg+\n7dTtufkxW/OJS6/O3qX+jb7ObY65VZ5wxmOze3l3XvSxP8znvvn5DVwlAACzYqbDdafTyT3PGLaG\nfPpGtoaMnX3K3fPTZzw2u5Z352WfeHU+fNXHNmiVAADMipkO18n+1pCP3ITWkLHvv8U987SzfjZb\nelvyus+8Me+67D2miAAAsM/Mh+v9rSE7b1JryNgZJ9whv3KPp+XmW4/POy97T/70s2/K3v4Nn6UN\nAMDsmflw3el0cvYZOzakNWTs1JudnP959tNzm2NulQ9f9bG8/BOvyXf2Xr8h1wYAoL1mPlwnyT3P\nODnJxrSGjB0zvz2/dPcn52477pLPfevzecFHX5bLr71iw64PAED7HBHheqNbQ8bme/P52bs8IQ+6\n9f3yteuvzgsuenle/9m/yrf3fGfD3gMAgPY4IsL1JFpDxrqdbn789g/LL939yTn1Zifng1/5cH7j\nQ8/PB778r+kPNi7IAwDQfEdEuE42dmrIau5w/O3yzHv+j/zEHR6e/qCfN9a35fkXvTSXXaNVBADg\nSHHEhOvvPfWY3PyYhQ1vDVmp1+3lgbf6wfzaff5X7nny9+WL1305L/joy/K6z7wxV1z7pYm8JwAA\nzXHEhOtha8joQJnLN7Y15GDHLhyTJ935J/OM73tqbnGzU/Lhqz6W5130kjz/Iy/NB6/8SPYs75no\n+wMAMB1HTLhO9reGXDSh1pCD3f640/Kr9/ql/MJZP5MzT7xjrrjuS3n9JX+ZZ134W/mr/3h7rvrO\n5qwDAIDNMTftBWymcWvIxz83bA3ZMjf53y26nW7ufPMzcuebn5Fv7P5mLrzyw/nglR/Oe7/0L3nv\nl/4lt9r+PbnjCafnTiecntOOvU3mukfUPwkAwEw5opJcp9PJPcpJ+fuPfDGfvvwbudvtT9zU9z9h\n6/F5+Pf+SB562wfl36/+dC788r/lc9/6fL543Zfz9194bxZ68zn9+NvljBNOzx1POD0nbTsxnU5n\nU9cIAMCNd0SF6yS55xnDcH3RJV/b9HA91uv28n0n3TXfd9Jds3tpMZd+6/P5zDf+I5d84z9y8f/f\n3t0HSVLXdxx/d8/M7t7e494Dh3ciz/7EIA8qjyKelEaipYJKmWjKaHxISlIay5QxRhO1tJSUxgpa\nakIiUWOsiihYwShYBkRBFEUMGPgRnk+QO+64R+5ud2a680fPzs7uze1xS+/N7s37Rc31c/dvmO/M\nfPo3vTOb7uT2TXcCMFQZYs2iw1m76Gnt25qFqxmqDvWk3YeiLM8Ya9apZ3UaWYOx1rCRNai3huPj\nzaxBI2/SzJo082Z7vJE1yfImWZ7RzDOyPCMja0/neV7cmBhmeU7O/v+wNiEp/k2K8YSEJOkYJgkp\naWvYmk7SiRtTpve6JaRJhUrHvInxCmmStKYrU5ZNXj8hpZKOH3N8v8V00tEOTxbnhzzPiyH5Ppcd\nDE+2XornyYFvJ0mzoe/C9TFrxi8N2XTQLg2ZzlB1kBNXnsCJK08AYPPuLdz1+N3cvfVefrPjEe7f\n9iD3bXtg0jYjg8tYPrSMkaFljAwWw+VDy1g2uIwlA4sYrg1Tm+eXl+R53g679azOaHOsFYLHJsab\nYwxsT9m8bQf1Zp3RbIyxZuuW1Rlr1hnLxqg3p4y3ltWzOs282eu72leSzhOAduBO2ycGnScLSUcg\nT6A1TDvGiwA1HqTac5KEWrVCo5FNhK6EyetMI28P80kzi+l8Yp18fM7Eep2htHOd9px86rLWPjq3\nyyf22rkNU6bbc/KJ/U895tS2dt6vqSG5W5A+FO2rAqYL5J3b7LXWPrabvs6mO1ZvzbwKDnzLuVJx\nCXOgLQfxpFVTdHkOJ8DXLvrsjHc5vxPYDCRJwmknrOZ7P32IH9/+W1586tpeN2mSFQtGeMHaM3jB\n2jMAGGvWeXTXBh7e+SgP73yEh3c+yoYnNnLftgfJp4TuTgNpjeHaMMPVBQzXFjBcHWagUmMgHaBW\nqTGQ1iaGaW1ST2Pnrf2iMyVI5HlORt7usc3yvNVT2yTLMhp5g0bWnBhmjaKXNy96gesdvcX1rEG9\nI/COZXXqzXqpb/a1tNa+/0PVQRYPLGrf91qlRi2tFuNplWp7WG0Pi1uFSjI+rLSHldZw757fid7j\ndjBsh8bJwbCbyYGrPUaeQ07W6v3u7A3PJj0WWZ6115tY1tmj3upp73gcm53rZM1iWzq3Kx7fjCn7\nmLLfvY7ZOkae5+19dq43fj+K+Xm7vpp5swiTedYOoe3/8n0HzATI8ok42f53mjew8e1aD0wxmLTG\nRHgfX9b5yULnOgkJJBOhrHO6PWfKCUIxlpKk3fYxceyOqa4nFpPm7HVSMb5dx/2ZMn+vXuD2TqYJ\nmFPXPQD7ep5P++zveByne52YuqxWqzA21mwv3fexOk+Y9t2qfR152teuaRfN5DVvUuWWYiaPY2vD\ng3esEtWq6ax9Re+B6P3/if6zr2fcU30skoP5Ed9TlD/22I5SdrRt5yjv/6ebGaimfOIdZzE8NP/O\nMZpZk62j29kyupUte7a2htvYUd/J7vpunmjsYld9N7sau9jd2NPr5u5TNam0w+xApdYOwbV0oBWG\nx08CWtOVgY7xGiuWLmF0V9YOzgOVGoOVAWrpAIOVYrqaVkmTvvpinL63atViynq90KHDulA31oW6\nWX/ImYUAAA2VSURBVLVq8Ywz9vxLlSVYumiQV5x5JN+64T6+c/MDXLTuuF436YBV0gorFoywYsHI\nftfN8oxdjd3tSyLGe43He4rrWWOvns2MjGY2cSbf2dM13iO272t1U6rJRG9vNa22e3qrk3qIn3ro\n9UVRkiTNJX0ZrgF+97Qj+OFtD/P9W9az7pS1rFq2oNdNmjVpkrKothBqvW6JJEnSoa1vPysfqFV4\n7bpjaTRzrrj+3l43R5IkSYeAvg3XAGecsJpj1izhlrs2cs9vtvW6OZIkSZrn+jpcJ0nC7593PABf\n/8H/dXy7gCRJknTg+jpcAxz39KWc9qzDuP+32/nZnRt63RxJkiTNY30frgFet+5YqpWEK66/l7G6\nPyoiSZKkmTFcA6uWLeClpx3B49tHufaW9b1ujiRJkuYpw3XLK848isXDNb5z84Ns2zna6+ZIkiRp\nHjJctwwPVbnghccwOtbkmzfc1+vmSJIkaR6atR+RCSGkwOeBk4FR4G0xxnumrDMMfB94a4zxrtlq\ny5N17slP47pbf8OP/+e3HLtmCS86ZW2vmyRJkqR5ZDZ7ri8AhmKMZwHvBz7duTCE8HzgBuDYWWzD\nAamkKRe/5jksWlDjq9fczR33be51kyRJkjSPzGa4Pgf4HkCM8Wbg+VOWDwIXAj3vse60emSYd732\nJNI04fNX3cFDG3b0ukmSJEmaJ2btshBgCdD5s4fNEEI1xtgAiDHeCBBCeNI7XLVqcakNnO44700T\nLvnKz/nst27n0+8+lxVLFxyUY+vAHay60PxiXagb60LdWBcq02yG6+1AZ7Wm48F6ph577OD1Ioc1\nS7joxcfyjevu5UNfvIn3v/G5LBiczf9dmolVqxYf1LrQ/GBdqBvrQt1YF+rmqZxwzeZlITcCLwcI\nIZwJ3D6Lx5oV55/+DNadupb1G3fyhW/fQTPLet0kSZIkzWGzGa6vBPaEEG4CPgO8J4TwhhDCO2bx\nmKVKkoQ3vvR4nnPMCu6473H+7dq7yfO8182SJEnSHJXMo7CY9+pjm92jDS752q08tHEnLzv9CF63\n7lgqqV8RPhf4cZ66sS7UjXWhbqwLdbNq1eJkptuaEJ+EBYNV3n3RyRw2soBrfraeT339Nn/FUZIk\nSXsxXD9JI4sH+Zs/Oo3nPXMVcf1WPnz5LcSHtvS6WZIkSZpDDNcHYHioyjsvPJHXn3ccO3fX+buv\n/5L/uvlBsvlzaY0kSZJmkeH6ACVJwstOfwbve8OpLF04wBXX38vnvnk7T+yp97ppkiRJ6jHD9Qwd\n//RlfPgtp3PCkSPcds8mPnL5Ldxy10Z7sSVJkvqY4fopWLJwgPe+/hReefZRPL59lC9cdQd/+y8/\nM2RLkiT1KX9y8ClK04QLzz2Gs59zOFff+AA/+fUGvnDVHaxduZBXnXM0zwurSJMZf5uLJEmS5hG/\n57pkG7bsaofsLM9Zu3IhLz/zSE45fqU/nz4L/H5SdWNdqBvrQt1YF+rmqXzPteF6lkwN2dVKwrOP\nWs5zn7mKU45fyZLhgV438ZDgi6K6sS7UjXWhbqwLdWO4nsM2btnFT369gVvvfoz1G3cCkCTwzKcv\n47lhFc8+coSnrVhImnrpyEz4oqhurAt1Y12oG+tC3TyVcO11CrPssJFhXn3O0bz6nKPZuGUXt969\niVvvfoy4fitx/VYABqopR6xexJGrF3Pk4Ys5cvVi1qxcSLXi35tKkiTNJ4brg+iwkWHOP+MZnH/G\nM9iyY5Rf3bOJ+x7ZzgOP7uD+R3Zw78Pb2+tWKwnLlwyxonVbvmSwGC4dYmTRIIsW1BgeqhrAJUmS\n5hDDdY+MLB5k3alrWXfqWgDqjSbrNz7Bgxt28OCj21m/cSebt+3hzgen/4n1wYEKC4eqLByqsXCo\nytBAlYFaymCtwkCtMjFerVCrplQqCdU0pVpJqFZa05WUNE2opglp61ZN0/Z4mkAlTUiTpGPexLIk\n6ZwHaZKQ+A0pkiSpD3nN9Rw3Vm+yZccom7fvYfO2PWzevoetO8fYtafOE3saPLGnzhO7i+GesWav\nm9uWUHxNYZK0Avh4+B4P4x2hPEnGA3kxr3N5khT7SpKEpLXj8fEkgVqtQqORTSwr/imWT2pQ0m5X\nx+Re9vV0mPZZMs1zaKbPrpn/FUX3LafOfbLnPgfrJKnswwzUqozVG+XudI7zdHb/agNV6mP9VRfa\nP+tCe0kSPvlnL/Sa60PVQK3C6uXDrF4+vN91G82MsXqT0XrGWKPJWD1jtN5krF6MN5rjt5xGltFo\nFOPNLKOZ5WRZTrN1y7KcZjOnmRfjWZ6TZx3TWU6W0142vnyveR3Tec6keXnHvGJ7yPOMPIc8L/Y1\nvk5Oaxwg7xhnYp1iHHLySan2YJw+TvsMPNCn5wwbPG9OkyVJOoQZrg8h1UpKtZIyPNTrlhw8M/kr\n7/19WnOoX9LSef+n75Gfwb5nsNFsfHjmX/+rm5UrF7Npk3WhyawLlc1wrb5zqIfn/em8/6X2uM98\no9KNn2hKnWpV60J7sy5UNqtJkiRJKonhWpIkSSqJ4VqSJEkqieFakiRJKonhWpIkSSqJ4VqSJEkq\nieFakiRJKonhWpIkSSqJ4VqSJEkqieFakiRJKonhWpIkSSqJ4VqSJEkqieFakiRJKonhWpIkSSqJ\n4VqSJEkqieFakiRJKonhWpIkSSqJ4VqSJEkqieFakiRJKonhWpIkSSqJ4VqSJEkqieFakiRJKonh\nWpIkSSqJ4VqSJEkqieFakiRJKonhWpIkSSqJ4VqSJEkqSZLnea/bIEmSJB0S7LmWJEmSSmK4liRJ\nkkpiuJYkSZJKYriWJEmSSmK4liRJkkpiuJYkSZJKYriWJEmSSlLtdQP2J4SQAp8HTgZGgbfFGO/p\nbavUCyGEGvAl4ChgEPgY8L/AvwI5cAdwcYwx61ET1UMhhMOAXwAvBRpYF30vhPBXwKuAAYr3kR9i\nXfS11vvIlyneR5rA2/H1oq+FEM4ALokxrgshHEeXWgghvB34E4pa+ViM8erp9jkfeq4vAIZijGcB\n7wc+3eP2qHf+ENgcY3whcD7wOeDvgQ+25iXAq3vYPvVI6w3zH4HdrVnWRZ8LIawDzgZeALwIOALr\nQvByoBpjPBv4KPBxrIu+FUJ4H/DPwFBr1l61EEI4HHgXxWvJy4BPhBAGp9vvfAjX5wDfA4gx3gw8\nv7fNUQ99A/hQazyhOIN8HkVvFMB3gZf0oF3qvU8BXwQeaU1bF3oZcDtwJfCfwNVYF4K7gWrrU/El\nQB3rop/dC7ymY7pbLZwO3BhjHI0xbgPuAU6abqfzIVwvAbZ1TDdDCHP+chaVL8a4M8a4I4SwGLgC\n+CCQxBjz1io7gKU9a6B6IoTwZuCxGOM1HbOtC62k6Iy5CPhT4GtAal30vZ0Ul4TcBVwGXIqvF30r\nxvhNihOscd1qYWoO3W+NzIdwvR1Y3DGdxhgbvWqMeiuEcARwHfDVGOO/A53XxS0GtvakYeqlPwZe\nGkK4HjgF+ApwWMdy66I/bQauiTGOxRgjsIfJb4jWRX96D0VdPJPib7m+THFN/jjror91yxRTc+h+\na2Q+hOsbKa6RIoRwJsXHfOpDIYTVwLXAX8YYv9Sa/cvWtZUAvwf8qBdtU+/EGM+NMb4oxrgOuA14\nE/Bd66Lv/Rg4P4SQhBDWAAuBH1gXfW8LE72QjwM1fB/RhG618DPghSGEoRDCUuAEij923Kf5cHnF\nlRS9UjdRXGf7lh63R73zAWAE+FAIYfza63cDl4YQBoA7KS4Xkd4LXGZd9K8Y49UhhHMp3hhT4GLg\nfqyLfvcZ4EshhB9R9Fh/APg51oUKe713xBibIYRLKYJ2Cvx1jHHPdDtJ8jyfbrkkSZKkJ2k+XBYi\nSZIkzQuGa0mSJKkkhmtJkiSpJIZrSZIkqSSGa0mSJKkkhmtJmmNCCHlruDSEcFWJ+72uY/y2svYr\nSZpguJakuWuE4lcny7JufCTGWOZ+JUkt8+FHZCSpX10KrAkhXBljvDCE8Cbgzyk6Rn4BXBxj3BNC\neKw1fThwGvB54ERgNRCB1wCXAIQQfhpjPCOEkMcYkxDCMHAZxU9BZ8CnYoxfCSG8GTgfWA4cA1wb\nY3znQbvnkjRP2XMtSXPXu4BHWsH6d4C3A2e3ep03An/RWm8l8MnW/LOAsRjjWcBxwALg5THGdwHE\nGM+YcowPA5tjjCcC5wEfDiGc1Fp2NvBa4CTglSGE58zS/ZSkQ4Y915I0P7wYOB64OYQAxU8339qx\n/KcAMcYbQgibQwgXA89qbbNomv2eB7y1te2mEMK3KS4f2Q7cFGPcARBCuI+iF1uSNA3DtSTNDxXg\nP8Z7oEMIi+h4DY8x7m7NfxXwUeAfgMsperWTafY79RPMpGO/ezrm5/vZjyQJLwuRpLmswUTQvR64\nMIRwWAghAb5Acf31VC+hCOGXA48C51IEc4BmCGFqp8p/0+q5DiGsBC5oHUuSNAOGa0mauzYAD4UQ\nrosx/gr4CEUY/jXF6/cnu2xzGfAHIYRfAt8CbgaObi37NvCrEMJQx/ofBZaHEG4HbgA+HmPsvNxE\nknQAkjzPe90GSZIk6ZBgz7UkSZJUEsO1JEmSVBLDtSRJklQSw7UkSZJUEsO1JEmSVBLDtSRJklQS\nw7UkSZJUkv8Hc5YWegXv3ToAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1247b3668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = obj_vals.plot()\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Objective value')\n",
    "ax.set_title('Objective function by iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_misclassification_errors(beta_results_df, X, y):\n",
    "    return beta_results_df.apply(lambda r: 1 - get_accuracy(r.values, X, y), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Train  Validation\n",
       "0  0.452381    0.611111\n",
       "1  0.095238    0.111111\n",
       "2  0.047619    0.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = pd.DataFrame({'Train': get_misclassification_errors(results, X_twoclasses_train, y_twoclasses_train),\n",
    "              'Validation': get_misclassification_errors(results, X_twoclasses_test, y_twoclasses_test)})\n",
    "errors[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11b30ef98>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAETCAYAAAAs4pGmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a85aa58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = errors.plot()\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Misclassification error')\n",
    "ax.set_title('Misclassification error by iteration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Corinne verification - using her code, do we see the same results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b397a23e9187>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m results_corinne, thetas_corinne = corinne.fastgradalgo(np.zeros(X_train.shape[1]),\n\u001b[0m\u001b[1;32m      2\u001b[0m                                        \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                        \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                        \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                        \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "results_corinne, thetas_corinne = corinne.fastgradalgo(np.zeros(X_train.shape[1]),\n",
    "                                       np.zeros(X_train.shape[1]),\n",
    "                                       1,\n",
    "                                       0.01,\n",
    "                                       100,\n",
    "                                       X_train,\n",
    "                                       y_train)\n",
    "results_corinne.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_corinne' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-222b882df108>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults_corinne\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'results_corinne' is not defined"
     ]
    }
   ],
   "source": [
    "results_corinne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_corinne' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-aa9076bceed5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorinne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_corinne\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'results_corinne' is not defined"
     ]
    }
   ],
   "source": [
    "corinne.objective_plot(results_corinne, 1, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_corinne' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-7013796952e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorinne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_misclassification_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_corinne\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'results_corinne' is not defined"
     ]
    }
   ],
   "source": [
    "corinne.compute_misclassification_error(results_corinne[100,:], X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-697e267bd4b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorinne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_misclassification_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "corinne.compute_misclassification_error(results.values[100,:], X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_corinne' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-0167550fc2f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults_corinne\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'results_corinne' is not defined"
     ]
    }
   ],
   "source": [
    "results_corinne[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00333503,  0.00057126,  0.00208173, ..., -0.00391905,\n",
       "       -0.00173726,  0.00095118])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.values[2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-fd640dd22682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlambduh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbeta_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtheta_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m eta_init = 1/(scipy.linalg.eigh(1/len(y_train)*X_train.T.dot(X_train), \n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "lambduh = 1\n",
    "d = X_train.shape[1]\n",
    "beta_init = np.zeros(d)\n",
    "theta_init = np.zeros(d)\n",
    "eta_init = 1/(scipy.linalg.eigh(1/len(y_train)*X_train.T.dot(X_train), \n",
    "                                eigvals=(d-1, d-1), eigvals_only=True)[0]+lambduh)\n",
    "maxiter = 300\n",
    "#betas_grad = corinne.graddescent(beta_init, lambduh, eta_init, maxiter, X_train, y_train)\n",
    "betas_fastgrad, thetas_fastgrad = corinne.fastgradalgo(beta_init, theta_init, lambduh, eta_init, maxiter,\n",
    "                                                       X_train, y_train)\n",
    "betas_fastgrad.shape\n",
    "#objective_plot(betas_grad, betas_fastgrad, lambduh, save_file='hw3_q1_part_h_output.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'betas_fastgrad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-19fc4c64c9bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbetas_fastgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'betas_fastgrad' is not defined"
     ]
    }
   ],
   "source": [
    "betas_fastgrad[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'betas_fastgrad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-f11ad1233967>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorinne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_misclassification_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbetas_fastgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'betas_fastgrad' is not defined"
     ]
    }
   ],
   "source": [
    "corinne.compute_misclassification_error(betas_fastgrad[-1,:], X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so, bottom line, we get the same results from our code as we do from Corinne's code, even when we use the exact same setup/init code (for ex, that sets eta_init using X.T.dot(X)), and when we use Corinne's code that does backtracking compared to our code, above, where we're not using backtracking. I think this is good enough for me to accept that w/ a lambda of 1 at least, logistic regression is way worse than just returning negative all the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we see when we use our backtracking impl?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-240589237066>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m results = fp.fastgradalgo(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mgrad_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_gradient_logistic_regression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mobj_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_objective_logistic_regression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     lam=1, max_iter=100)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "results = fp.fastgradalgo(\n",
    "    X_train, y_train, t_init=t_init, \n",
    "    grad_func = fp.compute_gradient_logistic_regression, \n",
    "    obj_func = fp.compute_objective_logistic_regression, \n",
    "    lam=1, max_iter=100)\n",
    "results[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-4bfc3fc392a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m results = fp.fastgradalgo(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mgrad_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_gradient_logistic_regression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mobj_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_objective_logistic_regression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     lam=1, max_iter=300, t_func=fp.backtracking)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "results = fp.fastgradalgo(\n",
    "    X_train, y_train, t_init=t_init, \n",
    "    grad_func = fp.compute_gradient_logistic_regression, \n",
    "    obj_func = fp.compute_objective_logistic_regression, \n",
    "    lam=1, max_iter=300, t_func=fp.backtracking)\n",
    "results[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-7a8f8f533245>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorinne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_misclassification_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "corinne.compute_misclassification_error(results.values[-1,:], X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-80b18520c499>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "1 - get_accuracy(results.values[-1,:], X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we get the same results w/ our own backtracking implementation. Note again that since our stopping criteria is _only_ the number of iterations, we don't automatically get any benefit from the improved step sizes that backtracking gives us. That said, we still in theory could get to the minimum faster, which means if we're manually setting max_iter based on experimentation, then backtracking could help.\n",
    "\n",
    "Finally, it's possible that backtracking adds enough processing time that we might want to run w/ it off if we're not going to use it. \n",
    "\n",
    "Although, backtracking might not _just_ give us better steps sizes leading to quicker convergence. It seems like it could also help us avoid the case where we _don't_ converge, if it keeps us from using a static/single step size that at some point means we're moving back and forth on our objective value without decreasing.\n",
    "\n",
    "Bottom line, if i care, I should probably benchmark, and watch for non-convergence being fixed when we use backtracking. I'll keep that in mind.\n",
    "\n",
    "Also, separately, it seems like my impl is way faster than Corinne's - I can run w/ 300 iterations in just a few secs while it takes her's well more than 10-20s it seems, anecdotally. It might be interesting to profile and see if a) this is really the case, and b) if so, why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-b9478be9d9a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorinne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "corinne.objective_plot(results.values, 1, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-ef3c85a2b25a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_objective_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcoefs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-ef3c85a2b25a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_objective_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcoefs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "[fp.compute_objective_logistic_regression(coefs, X_train, y_train, 1) for coefs in results.values][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Find the value of the regularization parameter λ using cross-validation; you may use scikit-learn’s built-in functions for this purpose. Train an L2-regularized logistic regression classiﬁer on the training set using your own fast gradient algorithm with that value of λ found by cross-validation. Plot, with diﬀerent colors, the misclassiﬁcation error on the training set and on the validation set vs iterations._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'corinne' from '/Users/andrewenfield/work/github/Data558/Kaggle/corinne.py'>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(fp)\n",
    "importlib.reload(corinne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas = [10 ** exponent for exponent in range(-5,2)]\n",
    "lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lams, scores = fp.cross_validate(10, X_twoclasses_train, y_twoclasses_train, lambdas, max_iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1e-05, 1.0),\n",
       " (0.0001, 1.0),\n",
       " (0.001, 1.0),\n",
       " (0.01, 1.0),\n",
       " (0.1, 1.0),\n",
       " (1, 1.0),\n",
       " (10, 0.95500000000000007)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(lams, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2038</th>\n",
       "      <th>2039</th>\n",
       "      <th>2040</th>\n",
       "      <th>2041</th>\n",
       "      <th>2042</th>\n",
       "      <th>2043</th>\n",
       "      <th>2044</th>\n",
       "      <th>2045</th>\n",
       "      <th>2046</th>\n",
       "      <th>2047</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-0.017001</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>0.010858</td>\n",
       "      <td>-0.010831</td>\n",
       "      <td>-0.002946</td>\n",
       "      <td>-0.001579</td>\n",
       "      <td>-0.006491</td>\n",
       "      <td>0.002654</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>-0.008531</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007823</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.012890</td>\n",
       "      <td>-0.031864</td>\n",
       "      <td>0.016663</td>\n",
       "      <td>0.023430</td>\n",
       "      <td>-0.024498</td>\n",
       "      <td>-0.038940</td>\n",
       "      <td>-0.007316</td>\n",
       "      <td>0.010297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-0.017045</td>\n",
       "      <td>0.001548</td>\n",
       "      <td>0.010883</td>\n",
       "      <td>-0.010866</td>\n",
       "      <td>-0.002948</td>\n",
       "      <td>-0.001584</td>\n",
       "      <td>-0.006525</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>-0.008564</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007850</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>0.012931</td>\n",
       "      <td>-0.031972</td>\n",
       "      <td>0.016698</td>\n",
       "      <td>0.023511</td>\n",
       "      <td>-0.024565</td>\n",
       "      <td>-0.039056</td>\n",
       "      <td>-0.007330</td>\n",
       "      <td>0.010338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>-0.017089</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>0.010909</td>\n",
       "      <td>-0.010900</td>\n",
       "      <td>-0.002950</td>\n",
       "      <td>-0.001590</td>\n",
       "      <td>-0.006559</td>\n",
       "      <td>0.002638</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>-0.008597</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007876</td>\n",
       "      <td>0.001735</td>\n",
       "      <td>0.012973</td>\n",
       "      <td>-0.032078</td>\n",
       "      <td>0.016733</td>\n",
       "      <td>0.023592</td>\n",
       "      <td>-0.024631</td>\n",
       "      <td>-0.039172</td>\n",
       "      <td>-0.007344</td>\n",
       "      <td>0.010378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 2048 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6     \\\n",
       "98  -0.017001  0.001543  0.010858 -0.010831 -0.002946 -0.001579 -0.006491   \n",
       "99  -0.017045  0.001548  0.010883 -0.010866 -0.002948 -0.001584 -0.006525   \n",
       "100 -0.017089  0.001552  0.010909 -0.010900 -0.002950 -0.001590 -0.006559   \n",
       "\n",
       "         7         8         9       ...         2038      2039      2040  \\\n",
       "98   0.002654  0.000364 -0.008531    ...    -0.007823  0.001720  0.012890   \n",
       "99   0.002646  0.000367 -0.008564    ...    -0.007850  0.001728  0.012931   \n",
       "100  0.002638  0.000370 -0.008597    ...    -0.007876  0.001735  0.012973   \n",
       "\n",
       "         2041      2042      2043      2044      2045      2046      2047  \n",
       "98  -0.031864  0.016663  0.023430 -0.024498 -0.038940 -0.007316  0.010297  \n",
       "99  -0.031972  0.016698  0.023511 -0.024565 -0.039056 -0.007330  0.010338  \n",
       "100 -0.032078  0.016733  0.023592 -0.024631 -0.039172 -0.007344  0.010378  \n",
       "\n",
       "[3 rows x 2048 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_best = fp.fastgradalgo(\n",
    "    X_twoclasses_train, y_twoclasses_train, t_init=t_init, \n",
    "    grad_func = fp.compute_gradient_logistic_regression, \n",
    "    obj_func = fp.compute_objective_logistic_regression, \n",
    "    lam=0, max_iter=max_iters)\n",
    "results_best[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp.get_accuracy(fp.get_final_coefs(results_best), X_twoclasses_test, y_twoclasses_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAETCAYAAAAs4pGmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c0a3710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fp.plot_misclassification_errors_by_iteration(results_best, X_twoclasses_train, X_twoclasses_test, \n",
    "                                              y_twoclasses_train, y_twoclasses_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'corinne' from '/Users/andrewenfield/work/github/Data558/Kaggle/corinne.py'>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(fp)\n",
    "importlib.reload(corinne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Pick k = 5 classes of your choice from the dataset. You may choose any subset of 5 classes among all classes of the dataset._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier_labels = ['086.Pacific_Loon','044.Frigatebird','095.Baltimore_Oriole',\n",
    "                     '154.Red_eyed_Vireo','188.Pileated_Woodpecker']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write a function that, for any class at hand, creates a training set with an equal number of examples from the class at hand and from the other classes. You may simply randomly pick the examples from the other classes._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42, 2048), (42,))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_incl086_train, X_incl086_test, labels_incl086_train, labels_incl086_test = fp.get_train_tst_balanced_set('086.Pacific_Loon', X_scaled, labels_train)\n",
    "y_incl086_train = np.where(labels_incl086_train == '086.Pacific_Loon', 1, -1)\n",
    "X_incl086_train.shape, y_incl086_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_For each class c, train an L2-regularized logistic regression classiﬁer using your own fast gradient algorithm with λc = 1. Display the confusion matrix. Which classes seem to be the most diﬃcult to classify?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.088888888888888906"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_rate, cm, actual_labels, predicted_labels = fp.get_results_for_lambdas(classifier_labels, X_scaled, \n",
    "                                                        labels_train, np.ones(5), random_state=100)\n",
    "error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdUAAAF2CAYAAAA4MQK3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZmZWiJOqmZlZIf8DzOhTG/pSzkgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b46b080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fp.plot_multiclass_confusion_matrix(cm, classifier_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at individual confusion matrices for each classifier, why do we always get a 0 in one corner? This means that we never have any false negatives - that is, we never predict -1 when it's actually 1. We do the other three. Why?\n",
    "\n",
    "So, this means that we never predict that it ISN'T the bird in question, when it is actually the bird in question. I tried w/ an array of thresholds from very low to very high, and while they changed some true negatives to false positives, they never changed any prediction to a false negative. Perhaps this is something that's not uncommon when you have a single 1-vs-rest classifier? For example, that classifier tends to see anything at all that looks remotely like the bird in question as the bird in question (leading to true positives when it IS the bird, and false positives when it's not), but does not - generally, or at all? - think that the bird in question is actually NOT the bird in question. I could also write the above or add in terms of precision and recall if I thought about it for a few more minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write a function that returns the ranked list of classes in terms of classiﬁcation diﬃculty using the confusion matrix. Compute the multi-class misclassiﬁcation error._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>086.Pacific_Loon</td>\n",
       "      <td>086.Pacific_Loon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>086.Pacific_Loon</td>\n",
       "      <td>086.Pacific_Loon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Actual         Predicted\n",
       "0  086.Pacific_Loon  086.Pacific_Loon\n",
       "1  086.Pacific_Loon  086.Pacific_Loon"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results_df = pd.DataFrame({'Actual': actual_labels,\n",
    "                                'Predicted': predicted_labels})\n",
    "test_results_df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Misclassification error</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>044.Frigatebird</th>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>095.Baltimore_Oriole</th>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>086.Pacific_Loon</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154.Red_eyed_Vireo</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188.Pileated_Woodpecker</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Misclassification error\n",
       "Actual                                          \n",
       "044.Frigatebird                         0.222222\n",
       "095.Baltimore_Oriole                    0.222222\n",
       "086.Pacific_Loon                        0.000000\n",
       "154.Red_eyed_Vireo                      0.000000\n",
       "188.Pileated_Woodpecker                 0.000000"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_by_class = test_results_df.groupby(['Actual']).agg(lambda g: 1 - accuracy_score(g['Actual'], g['Predicted']))\n",
    "accuracy_by_class.rename(columns={'Predicted': 'Misclassification error'}, inplace=True)\n",
    "accuracy_by_class.sort_values(['Misclassification error'], ascending=False, inplace=True)\n",
    "\n",
    "accuracy_by_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall multi-class misclassification error: 8.9%\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall multi-class misclassification error: {:.1%}\".format(1 - accuracy_score(actual_labels, predicted_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Find the values of the regularization parameters λ1,...,λk for the classiﬁers using a hold-out validation set strategy. Deﬁne a grid of values Λ for each parameter λc with c = 1,...,k. For each setting of the regularization parameters λ1,...,λk, where each λc can take values in Λ (independently), train all your k = 5 classiﬁers and save the multi-class misclassiﬁcation error on the validation set for each setting of the regularization parameters λ1,...,λk._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** write code that creates a grid and runs for each combo, and then that outputs and saves the misclassification error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.001, 0.01, 0.1, 1]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas = [10 ** exponent for exponent in range(-3,1)]\n",
    "lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_combinations = list(itertools.product(lambdas, repeat=5))\n",
    "len(lambda_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.001, 0.001, 0.001, 0.001, 0.001),\n",
       " (0.001, 0.001, 0.001, 0.001, 0.01),\n",
       " (0.001, 0.001, 0.001, 0.001, 0.1)]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_combinations[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.19999999999999996, 0.28888888888888886, 0.17777777777777781]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[fp.get_results_for_lambdas(classifier_labels, X_scaled, labels_train, lams)[0] for lams in lambda_combinations[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "type(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Need to create train/test sets - one for each label, I think - OUTSIDE the get_results call, and pass it in, so each set of lambdas gets a chance w/ the same set of train and test data. (Might also speed things up at least a little bit, since we won't have to get new sets each time.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at 2017-05-28 22:34:33.608803.\n",
      "Processing lambdas with index 0...\n",
      "Finished at 2017-05-28 22:35:13.140013. Processed 10 sets of lambdas\n"
     ]
    }
   ],
   "source": [
    "print(\"Started at {}.\".format(datetime.now()))\n",
    "\n",
    "misclassification_errors = []\n",
    "\n",
    "try:\n",
    "    for index, lams in enumerate(lambda_combinations[:10]):\n",
    "        if (index % 10 == 0):\n",
    "            print('Processing lambdas with index {}...'.format(index))\n",
    "\n",
    "        error = fp.get_results_for_lambdas(classifier_labels, X_scaled, labels_train, lams)[0]\n",
    "        misclassification_errors.append(error)\n",
    "\n",
    "except:\n",
    "    raise()\n",
    "finally:\n",
    "    print(\"Finished at {}. Processed {} sets of lambdas\".format(datetime.now(), index+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.044444444444444398,\n",
       " 0.044444444444444398,\n",
       " 0.088888888888888906,\n",
       " 0.11111111111111116,\n",
       " 0.22222222222222221,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.31111111111111112,\n",
       " 0.33333333333333337]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassification_errors.sort()\n",
    "misclassification_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_misclassification_error_for_lambdas(lambdas):\n",
    "    return lambdas, fp.get_results_for_lambdas(classifier_labels, X_scaled, labels_train, lambdas)[0]\n",
    "\n",
    "# uses already-set (global - boo!) sets_per_labels, since this doesn't change at all for all the runs\n",
    "# and to avoid having to update to take a tuple/deal with multiple arg unpacking (for now at least)\n",
    "def get_misclassification_error_for_lambdas_with_shared_sets(lambdas):\n",
    "    return lambdas, fp.get_results_for_lambdas(classifier_labels, X_scaled, labels_train, lambdas,\n",
    "                                               sets_for_labels=sets_per_labels)[0]\n",
    "\n",
    "def create_and_run_jobs(lambda_combinations, worker_job):\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "         return executor.map(worker_job, lambda_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at 2017-05-29 09:47:55.743522.\n",
      "Finished at 2017-05-29 09:48:03.926263. Processed 3 sets of lambdas.\n"
     ]
    }
   ],
   "source": [
    "print(\"Started at {}.\".format(datetime.now()))\n",
    "\n",
    "all_results = []\n",
    "\n",
    "results = create_and_run_jobs(lambda_combinations[:3], get_misclassification_error_for_lambdas)\n",
    "for result in results:\n",
    "    all_results.append(result)\n",
    "    \n",
    "print(\"Finished at {}. Processed {} sets of lambdas.\".format(datetime.now(), len(all_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0.001, 0.001, 0.001, 0.001, 0.001), 0.26666666666666672),\n",
       " ((0.001, 0.001, 0.001, 0.001, 0.01), 0.26666666666666672),\n",
       " ((0.001, 0.001, 0.001, 0.001, 0.1), 0.17777777777777781)]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pickle.dump(all_results, open('all_results', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.022222222222222254,\n",
       " 0.022222222222222254,\n",
       " 0.022222222222222254,\n",
       " 0.022222222222222254,\n",
       " 0.044444444444444398,\n",
       " 0.044444444444444398,\n",
       " 0.044444444444444398,\n",
       " 0.044444444444444398,\n",
       " 0.044444444444444398,\n",
       " 0.044444444444444398,\n",
       " 0.044444444444444398,\n",
       " 0.044444444444444398,\n",
       " 0.044444444444444398,\n",
       " 0.044444444444444398,\n",
       " 0.044444444444444398,\n",
       " 0.044444444444444398,\n",
       " 0.044444444444444398,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.066666666666666652,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.088888888888888906,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.11111111111111116,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.1333333333333333,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.15555555555555556,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.17777777777777781,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.19999999999999996,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.22222222222222221,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.24444444444444446,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.26666666666666672,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.28888888888888886,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.31111111111111112,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.33333333333333337,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.35555555555555551,\n",
       " 0.37777777777777777,\n",
       " 0.37777777777777777,\n",
       " 0.37777777777777777,\n",
       " 0.37777777777777777,\n",
       " 0.37777777777777777,\n",
       " 0.37777777777777777,\n",
       " 0.37777777777777777,\n",
       " 0.37777777777777777,\n",
       " 0.37777777777777777,\n",
       " 0.37777777777777777,\n",
       " 0.37777777777777777,\n",
       " 0.37777777777777777,\n",
       " 0.37777777777777777,\n",
       " 0.37777777777777777,\n",
       " 0.37777777777777777,\n",
       " 0.37777777777777777,\n",
       " 0.37777777777777777,\n",
       " 0.37777777777777777,\n",
       " 0.37777777777777777,\n",
       " 0.37777777777777777,\n",
       " 0.37777777777777777,\n",
       " 0.37777777777777777,\n",
       " 0.37777777777777777,\n",
       " 0.40000000000000002,\n",
       " 0.40000000000000002,\n",
       " 0.40000000000000002,\n",
       " 0.40000000000000002,\n",
       " 0.40000000000000002,\n",
       " 0.40000000000000002,\n",
       " 0.40000000000000002,\n",
       " 0.40000000000000002,\n",
       " 0.40000000000000002,\n",
       " 0.40000000000000002,\n",
       " 0.40000000000000002,\n",
       " 0.40000000000000002,\n",
       " 0.40000000000000002,\n",
       " 0.40000000000000002,\n",
       " 0.40000000000000002,\n",
       " 0.40000000000000002,\n",
       " 0.40000000000000002,\n",
       " 0.40000000000000002,\n",
       " 0.42222222222222228,\n",
       " 0.42222222222222228,\n",
       " 0.42222222222222228,\n",
       " 0.42222222222222228,\n",
       " 0.42222222222222228,\n",
       " ...]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = [error for _, error in all_results]\n",
    "foo.sort()\n",
    "foo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, with the ability to generate the train-test sets just once, so that a) most importantly, each fastgradalgo run gets to use the same particular set of train/test data, so one doesn't have an easier or harder time than another, and b) it might be at least a bit faster since we only have to generate the train-test data once rather than for each of the runs for a set of five lambdas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the worker func requires the train-test sets to pointed to by the sets_per_labels variable - argh, global state (see above)\n",
    "sets_per_labels = [fp.get_train_tst_balanced_set(classifier_label, X_scaled, labels_train) for classifier_label in classifier_labels]\n",
    "len(sets_per_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at 2017-05-29 09:56:55.569739.\n",
      "Finished at 2017-05-29 10:41:34.992012. Processed 1024 sets of lambdas.\n"
     ]
    }
   ],
   "source": [
    "print(\"Started at {}.\".format(datetime.now()))\n",
    "\n",
    "all_results_shared_sets = []\n",
    "\n",
    "results = create_and_run_jobs(lambda_combinations, get_misclassification_error_for_lambdas_with_shared_sets)\n",
    "for result in results:\n",
    "    all_results_shared_sets.append(result)\n",
    "    \n",
    "print(\"Finished at {}. Processed {} sets of lambdas.\".format(datetime.now(), len(all_results_shared_sets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0.001, 0.001, 0.001, 0.001, 0.001), 0.1333333333333333),\n",
       " ((0.001, 0.001, 0.001, 0.001, 0.01), 0.22222222222222221),\n",
       " ((0.001, 0.001, 0.001, 0.001, 0.1), 0.26666666666666672)]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results_shared_sets[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pickle.dump(all_results_shared_sets, open('all_results_shared_sets', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.01509434,  0.00552825,  0.0055491 , ..., -0.00297287,\n",
       "        -0.00921239, -0.00852349]),\n",
       " array([[ 5.09265533,  4.20112905, -1.62171833, ..., -0.215201  ,\n",
       "         -1.41351453, -0.17103363],\n",
       "        [ 0.11460979,  5.61620863, -1.14607335, ...,  1.44764577,\n",
       "         -0.16874614, -1.23431923],\n",
       "        [-0.82787628,  0.08774076, -1.29261557, ...,  0.44963873,\n",
       "         -0.9061965 ,  0.15335795],\n",
       "        ..., \n",
       "        [-0.44432821,  0.64231437, -0.54492884, ..., -0.64411074,\n",
       "         -0.62482926, -0.34675179],\n",
       "        [-0.30679252, -0.10385079, -0.42422159, ..., -0.29760614,\n",
       "         -0.73200947,  0.17702185],\n",
       "        [-1.47711076, -0.03799772, -1.30542734, ...,  0.90678552,\n",
       "         -0.56708922,  0.16192286]]),\n",
       " array(['188.Pileated_Woodpecker', '188.Pileated_Woodpecker',\n",
       "        '188.Pileated_Woodpecker', '188.Pileated_Woodpecker',\n",
       "        '188.Pileated_Woodpecker', '188.Pileated_Woodpecker',\n",
       "        '188.Pileated_Woodpecker', '188.Pileated_Woodpecker',\n",
       "        '188.Pileated_Woodpecker'], \n",
       "       dtype='<U29'))"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foo, bar, baz = zip(*results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['086.Pacific_Loon', '086.Pacific_Loon', '086.Pacific_Loon',\n",
       "        '086.Pacific_Loon', '086.Pacific_Loon', '086.Pacific_Loon',\n",
       "        '086.Pacific_Loon', '086.Pacific_Loon', '086.Pacific_Loon'], \n",
       "       dtype='<U29'),\n",
       " array(['044.Frigatebird', '044.Frigatebird', '044.Frigatebird',\n",
       "        '044.Frigatebird', '044.Frigatebird', '044.Frigatebird',\n",
       "        '044.Frigatebird', '044.Frigatebird', '044.Frigatebird'], \n",
       "       dtype='<U29'),\n",
       " array(['095.Baltimore_Oriole', '095.Baltimore_Oriole',\n",
       "        '095.Baltimore_Oriole', '095.Baltimore_Oriole',\n",
       "        '095.Baltimore_Oriole', '095.Baltimore_Oriole',\n",
       "        '095.Baltimore_Oriole', '095.Baltimore_Oriole',\n",
       "        '095.Baltimore_Oriole'], \n",
       "       dtype='<U29'),\n",
       " array(['154.Red_eyed_Vireo', '154.Red_eyed_Vireo', '154.Red_eyed_Vireo',\n",
       "        '154.Red_eyed_Vireo', '154.Red_eyed_Vireo', '154.Red_eyed_Vireo',\n",
       "        '154.Red_eyed_Vireo', '154.Red_eyed_Vireo', '154.Red_eyed_Vireo'], \n",
       "       dtype='<U29'),\n",
       " array(['188.Pileated_Woodpecker', '188.Pileated_Woodpecker',\n",
       "        '188.Pileated_Woodpecker', '188.Pileated_Woodpecker',\n",
       "        '188.Pileated_Woodpecker', '188.Pileated_Woodpecker',\n",
       "        '188.Pileated_Woodpecker', '188.Pileated_Woodpecker',\n",
       "        '188.Pileated_Woodpecker'], \n",
       "       dtype='<U29'))"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'corinne' from '/Users/andrewenfield/work/github/Data558/Kaggle/corinne.py'>"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(fp)\n",
    "importlib.reload(corinne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Find the optimal value of the regularization parameters λ1,...,λk based on the validation error. Display the confusion matrix for this setting of the regularization parameters._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 1, 1, 1, 1), 0.088888888888888906),\n",
       " ((0.01, 0.01, 1, 0.01, 0.01), 0.11111111111111116),\n",
       " ((0.01, 0.01, 1, 0.1, 0.01), 0.11111111111111116)]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(all_results_shared_sets, key=lambda t: t[1])[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1, 1, 1)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So, here's the best lambda values:\n",
    "lambdas_best = sorted(all_results_shared_sets, key=lambda t: t[1])[0][0]\n",
    "lambdas_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# and we'll get the coefs/get the classifier (we didn't save the coef values from the run, so \n",
    "# will get them now)\n",
    "results = [fp.get_classifier_for_label(classifier_label, X_scaled, labels_train, lam, sets_for_this_label) for\n",
    "                                 classifier_label, lam, sets_for_this_label in zip(classifier_labels, lambdas_best, sets_for_labels)]\n",
    "#final_coefs, X_test, labels_test = results\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "misclassification_error, cm, _, _ = fp.get_results_for_lambdas(classifier_labels, X_scaled, labels_train, \n",
    "                                                               lambdas_best, sets_for_labels=sets_per_labels)\n",
    "misclassification_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fp.plot_multiclass_confusion_matrix(cm, classifier_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Seven"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In this exercise, you are going to train support vector machines (SVMs) using scikit-learn and the data competition project dataset. You will consider here all classes in the dataset. You may work on this exercise on your own computer ﬁrst. Note, however, that you need AWS to run the experiments for the last two parts of this exercise._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In a one-vs-one fashion, for each pairs of classes, train a linear SVM classiﬁer using scikit-learn’s function LinearSVC, with the default value for the regularization parameter. Compute the multi-class misclassiﬁcation error obtained using these classiﬁers trained in a one-vs-one fashion._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4320"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled[:10,:3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 s, sys: 1.89 s, total: 17.9 s\n",
      "Wall time: 26.2 s\n"
     ]
    }
   ],
   "source": [
    "# how_many = 30 * 144\n",
    "# num_of_features = 256\n",
    "# %time ovo = OneVsOneClassifier(LinearSVC(random_state=0), n_jobs=-1).fit(X_scaled[:how_many, :num_of_features], labels_train[:how_many])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 23s, sys: 33.7 s, total: 1min 56s\n",
      "Wall time: 1min 50s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how_many_predict = 30 * 144\n",
    "# %time accuracy_score(labels_train[:how_many_predict], ovo.predict(X_scaled[:how_many_predict, :num_of_features]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so from trying above with num_of_features = 2, the absolute minimum time this could take - on my laptop - is about 5s for training 144 choose 2 classifiers, and 11s to predict 4.3k results using these classifiers, WHEN using only two features (instead of 2048). This compares to about 6-7hrs for all 2048 features, judging from the experience of others. If I bump this up to 10 features, it takes 6s to train and 13s to predict. With 30 features, 5s to train and 18s to predict. 64 features: 7s to train, 33s to predict. 256 features: 22.6s to train, 1:48 to predict.\n",
    "\n",
    "I'll give 256 features a try, with PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4320, 256)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_pca_components = 256\n",
    "\n",
    "pca = PCA(n_components=num_of_pca_components)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "X_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.85799174])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how much variance can we explain w/ all of the components?\n",
    "np.cumsum(pca.explained_variance_ratio_)[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.4 s, sys: 1.67 s, total: 16.1 s\n",
      "Wall time: 22.7 s\n"
     ]
    }
   ],
   "source": [
    "how_many = 30 * 144\n",
    "%time ovo_pca = OneVsOneClassifier(LinearSVC(random_state=0), n_jobs=-1).fit(X_pca[:how_many], labels_train[:how_many])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "how_many_predict = 30 * 72\n",
    "%time accuracy_score(labels_train[:how_many_predict], ovo_pca.predict(X_pca[:how_many_predict, :num_of_features]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "With a 128 component PCA, 11s to train and 42s to predict 4.3k observations. With a 256 component PCA, 25s to train and 90s to predict. I get 76% of variance explained with 128 components compared to 86% of variance explained with 256 components. It looks like predicting is linear as predicting half the observations with 256 components instead of the full 4.3k takes 44s (instead of 90). \n",
    "\n",
    "The training time is small (25s) and I won't be predicting a full 4.3k likely, as I'll probably only predict on a hold out set, so I could go ahead with 256 components for now. That said, do I really care about the accuracy? Not really - I'll go with 128 and bump it up later if I have to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we need a train/test split so we can compute the misclassification accuracy all up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_pca_train, X_pca_test, y_pca_train, y_pca_test = model_selection.train_test_split(X_pca, labels_train, random_state=42)\n",
    "X_pca_train.shape, X_pca_test.shape, y_pca_train.shape, y_pca_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ovo_pca = OneVsOneClassifier(LinearSVC(random_state=0), n_jobs=-1).fit(X_pca_train, y_pca_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ovo_pca_predicted_values = ovo_pca.predict(X_pca_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1 - accuracy_score(y_pca_test, ovo_pca_predicted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_pca_test, ovo_pca_predicted_values)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fp.plot_multiclass_confusion_matrix(cm, np.unique(labels_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 128 components, I get a misclassification error of 34%. With 256 components, I get a misclassification error of 33%. This at least says it's cool to stay with 128 components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In a one-vs-rest fashion, for each class, train a linear SVM classiﬁer using scikit-learn’s function LinearSVC, with the default value for λc. Compute the multi-class misclassiﬁcation error obtained using these classiﬁers trained in a one-vs-rest fashion._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Using the option multi class=’crammer singer’ in scikitlearn’s function LinearSVC, train a multi-class linear SVM classiﬁer using the default value for the regularization parameter. Compute the multi-class misclassiﬁcation error obtained using this multiclass linear SVM classiﬁer._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVMs for multi-class classiﬁcation\n",
    "\n",
    "_Redo all questions above now tuning the regularization parameters using cross-validation._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In a one-vs-one fashion, for each pairs of classes, train a linear SVM classiﬁer using scikit-learn’s function LinearSVC, with the default value for the regularization parameter. Compute the multi-class misclassiﬁcation error obtained using these classiﬁers trained in a one-vs-one fashion._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In a one-vs-rest fashion, for each class, train a linear SVM classiﬁer using scikit-learn’s function LinearSVC, with the default value for λc. Compute the multi-class misclassiﬁcation error obtained using these classiﬁers trained in a one-vs-rest fashion._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Using the option multi class=’crammer singer’ in scikitlearn’s function LinearSVC, train a multi-class linear SVM classiﬁer using the default value for the regularization parameter. Compute the multi-class misclassiﬁcation error obtained using this multiclass linear SVM classiﬁer._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel SVMs for multi-class classiﬁcation\n",
    "\n",
    "_Redo all questions above now using the polynomial kernel of order 2 (and tuning the regularization parameters using cross-validation)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In a one-vs-one fashion, for each pairs of classes, train a linear SVM classiﬁer using scikit-learn’s function LinearSVC, with the default value for the regularization parameter. Compute the multi-class misclassiﬁcation error obtained using these classiﬁers trained in a one-vs-one fashion._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In a one-vs-rest fashion, for each class, train a linear SVM classiﬁer using scikit-learn’s function LinearSVC, with the default value for λc. Compute the multi-class misclassiﬁcation error obtained using these classiﬁers trained in a one-vs-rest fashion._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Using the option multi class=’crammer singer’ in scikitlearn’s function LinearSVC, train a multi-class linear SVM classiﬁer using the default value for the regularization parameter. Compute the multi-class misclassiﬁcation error obtained using this multiclass linear SVM classiﬁer._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
